#!/usr/bin/env python3
"""
å°ç£æ³•è¦ RAG çŸ¥è­˜æª¢ç´¢ç³»çµ±
æ”¯æŒé£Ÿå“å®‰å…¨è¡›ç”Ÿç®¡ç†æ³•ã€å‹å‹•åŸºæº–æ³•åŠå¤šæ³•è¦æ•´åˆæŸ¥è©¢
ä¸»è¦CLIä»‹é¢ç¨‹å¼
"""

import os
import argparse
import sys
import time
from pathlib import Path
from typing import Optional, List
import json
from datetime import datetime

# Fix Windows console encoding issues
if sys.platform == "win32":
    import locale
    # Set console encoding to UTF-8
    if hasattr(sys.stdout, 'reconfigure'):
        sys.stdout.reconfigure(encoding='utf-8', errors='replace')
    if hasattr(sys.stderr, 'reconfigure'):
        sys.stderr.reconfigure(encoding='utf-8', errors='replace')
    # Set environment variable for subprocess calls
    os.environ['PYTHONIOENCODING'] = 'utf-8'

# Disable ChromaDB telemetry completely (multiple methods for compatibility)
os.environ['ANONYMIZED_TELEMETRY'] = 'False'
os.environ['CHROMA_TELEMETRY'] = 'False'
os.environ['CHROMA_SERVER_NOFILE'] = '65536'
os.environ['CHROMA_SERVER_CORS_ALLOW_ORIGINS'] = '["*"]'

# Additional telemetry disabling and log level control
try:
    import chromadb
    from chromadb.config import Settings as ChromaSettings
    import logging

    # Set global ChromaDB settings to disable telemetry
    chromadb.configure(anonymized_telemetry=False)

    # Suppress ChromaDB telemetry and verbose logging
    logging.getLogger('chromadb.telemetry.product.posthog').setLevel(logging.CRITICAL)
    logging.getLogger('chromadb').setLevel(logging.WARNING)

except Exception:
    pass  # Continue if chromadb not available yet

from rich.console import Console
from rich.table import Table
from rich.panel import Panel
from rich.prompt import Prompt, Confirm
from rich.progress import Progress, SpinnerColumn, TextColumn
from rich.text import Text
from rich import print as rprint

from src.legal_food_safety_fetcher import FoodSafetyActFetcher
from src.labor_law_fetcher import LaborLawFetcher
from src.civil_law_fetcher import CivilLawFetcher
from src.legal_basic_processor import LegalDocumentProcessor
from src.legal_enhanced_processor import EnhancedLegalProcessor
from src.index_builder import LegalIndexBuilder
from src.legal_single_domain_rag import LegalRAGSystem
from src.query_router import QueryRouter
from src.legal_multi_domain_rag import AdvancedRAGSystem
from src.monitoring import WandbMonitor, initialize_global_monitor, create_config_from_env


class LegalRAGCLI:
    """å°ç£æ³•è¦RAGç³»çµ±çš„CLIä»‹é¢ï¼Œæ”¯æŒé£Ÿå“å®‰å…¨æ³•ã€å‹åŸºæ³•åŠå¤šæ³•è¦æ•´åˆæŸ¥è©¢ï¼Œæ•´åˆ W&B ç›£æ§"""

    def __init__(self, enable_monitoring: bool = True):
        # Initialize console with UTF-8 for Windows compatibility
        try:
            self.console = Console(force_terminal=True, width=None)
            # Try to configure Rich console for UTF-8 on Windows
            if sys.platform == "win32":
                self.console._file = sys.stdout
        except Exception:
            self.console = Console()
        self.food_safety_data_file = Path("data/food_safety_act.json")
        self.labor_law_data_file = Path("data/labor_standards_act.json")
        self.civil_law_data_file = Path("data/civil_code.json")
        self.env_file = Path(".env")

        # ç›£æ§è¨­ç½®
        self.enable_monitoring = enable_monitoring
        self.monitor: Optional[WandbMonitor] = None
        self.session_start_time = time.time()

        # ç³»çµ±çµ„ä»¶
        self.rag_system: Optional[LegalRAGSystem] = None
        self.query_router: Optional[QueryRouter] = None
        self.advanced_rag_system: Optional[AdvancedRAGSystem] = None
        self.index_builder: Optional[LegalIndexBuilder] = None

    def setup_monitoring(self):
        """è¨­ç½® W&B ç›£æ§"""
        if not self.enable_monitoring:
            return

        try:
            # è¼‰å…¥ç’°å¢ƒè®Šæ•¸
            from dotenv import load_dotenv
            load_dotenv()

            # æª¢æŸ¥ W&B è¨­å®š
            wandb_mode = os.getenv("WANDB_MODE", "online")
            wandb_project = os.getenv("WANDB_PROJECT", "food-safety-rag")

            if wandb_mode == "disabled":
                self.console.print("[yellow]W&B ç›£æ§å·²åœç”¨[/yellow]")
                self.enable_monitoring = False
                return

            # åˆå§‹åŒ–ç›£æ§å™¨
            try:
                self.monitor = WandbMonitor(
                    project_name=wandb_project,
                    mode=wandb_mode,
                    tags=["cli-session", "food-safety"]
                )
            except Exception as e:
                self.console.print(f"[yellow]ç›£æ§å™¨åˆå§‹åŒ–å¤±æ•—: {e}[/yellow]")
                self.enable_monitoring = False
                self.monitor = None
                return

            # è¨­å®šç‚ºå…¨åŸŸç›£æ§å™¨
            try:
                initialize_global_monitor(
                    project_name=wandb_project,
                    mode=wandb_mode,
                    tags=["cli-session", "food-safety"]
                )
            except Exception as e:
                self.console.print(f"[yellow]å…¨åŸŸç›£æ§å™¨è¨­å®šå¤±æ•—: {e}[/yellow]")

            # åˆå§‹åŒ– W&B run
            if self.monitor:
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                config = create_config_from_env()
                config["session_type"] = "cli"

                self.monitor.init_run(
                    run_name=f"cli_session_{timestamp}",
                    config=config
                )

                self.console.print(f"[green]W&B ç›£æ§å·²å•Ÿç”¨ - å°ˆæ¡ˆ: {wandb_project}[/green]")

        except Exception as e:
            self.console.print(f"[yellow]W&B ç›£æ§åˆå§‹åŒ–å¤±æ•—: {e}[/yellow]")
            self.enable_monitoring = False
            self.monitor = None

    def check_environment(self) -> bool:
        """æª¢æŸ¥ç’°å¢ƒè¨­ç½®"""
        issues = []

        # æª¢æŸ¥.envæª”æ¡ˆ
        if not self.env_file.exists():
            issues.append("[FAIL] æœªæ‰¾åˆ° .env æª”æ¡ˆã€‚è«‹è¤‡è£½ .env.template ä¸¦è¨­å®šæ‚¨çš„ OpenAI API key")
        else:
            # æª¢æŸ¥API key
            from dotenv import load_dotenv
            load_dotenv()
            if not os.getenv("OPENAI_API_KEY"):
                issues.append("[FAIL] .env æª”æ¡ˆä¸­æœªè¨­å®š OPENAI_API_KEY")

        # æª¢æŸ¥è³‡æ–™æª”æ¡ˆ
        if not self.food_safety_data_file.exists() and not self.labor_law_data_file.exists() and not self.civil_law_data_file.exists():
            issues.append("[FAIL] æœªæ‰¾åˆ°æ³•è¦è³‡æ–™æª”æ¡ˆï¼Œéœ€è¦å…ˆä¸‹è¼‰æ³•è¦å…§å®¹ï¼ˆé£Ÿå“å®‰å…¨æ³•ã€å‹åŸºæ³•æˆ–æ°‘æ³•ï¼‰")

        if issues:
            self.console.print("\n[red]ç’°å¢ƒè¨­ç½®å•é¡Œï¼š[/red]")
            for issue in issues:
                self.console.print(f"  {issue}")

            # è¨˜éŒ„ç’°å¢ƒæª¢æŸ¥å¤±æ•—
            if self.monitor:
                self.monitor.log_metrics({
                    "environment_check_passed": False,
                    "environment_issues_count": len(issues)
                })

            return False

        self.console.print("[OK] ç’°å¢ƒæª¢æŸ¥é€šé")

        # è¨˜éŒ„ç’°å¢ƒæª¢æŸ¥æˆåŠŸ
        if self.monitor:
            self.monitor.log_metrics({
                "environment_check_passed": True,
                "environment_issues_count": 0
            })

        return True

    def setup_food_safety_data(self) -> bool:
        """è¨­ç½®é£Ÿå“å®‰å…¨æ³•è³‡æ–™ï¼ˆä¸‹è¼‰æ³•è¦ï¼‰"""
        if self.food_safety_data_file.exists():
            if not Confirm.ask("é£Ÿå“å®‰å…¨æ³•è³‡æ–™æª”æ¡ˆå·²å­˜åœ¨ï¼Œæ˜¯å¦é‡æ–°ä¸‹è¼‰ï¼Ÿ"):
                return True

        self.console.print("\n[yellow]é–‹å§‹ä¸‹è¼‰å°ç£é£Ÿå“å®‰å…¨è¡›ç”Ÿç®¡ç†æ³•...[/yellow]")

        try:
            with Progress(
                SpinnerColumn(),
                TextColumn("[progress.description]{task.description}"),
                console=self.console
            ) as progress:

                task = progress.add_task("ä¸‹è¼‰æ³•è¦å…§å®¹...", total=None)

                fetcher = FoodSafetyActFetcher(delay=1.0)
                articles = fetcher.fetch_all_articles()

                progress.update(task, description="å„²å­˜è³‡æ–™...")
                fetcher.save_to_json(str(self.food_safety_data_file))

                progress.update(task, description="å®Œæˆï¼")

            self.console.print(f"[OK] æˆåŠŸä¸‹è¼‰ {len(articles)} æ¢é£Ÿå“å®‰å…¨æ³•")
            return True

        except Exception as e:
            self.console.print(f"[FAIL] é£Ÿå“å®‰å…¨æ³•ä¸‹è¼‰å¤±æ•—: {e}")
            return False

    def setup_labor_law_data(self) -> bool:
        """è¨­ç½®å‹åŸºæ³•è³‡æ–™ï¼ˆä¸‹è¼‰æ³•è¦ï¼‰"""
        if self.labor_law_data_file.exists():
            if not Confirm.ask("å‹åŸºæ³•è³‡æ–™æª”æ¡ˆå·²å­˜åœ¨ï¼Œæ˜¯å¦é‡æ–°ä¸‹è¼‰ï¼Ÿ"):
                return True

        self.console.print("\n[yellow]é–‹å§‹ä¸‹è¼‰å°ç£å‹å‹•åŸºæº–æ³•...[/yellow]")

        try:
            with Progress(
                SpinnerColumn(),
                TextColumn("[progress.description]{task.description}"),
                console=self.console
            ) as progress:

                task = progress.add_task("ä¸‹è¼‰æ³•è¦å…§å®¹...", total=None)

                fetcher = LaborLawFetcher(delay=1.5)  # ç¨é•·çš„å»¶é²ä»¥ç¤ºå°Šé‡
                articles = fetcher.fetch_all_articles()

                progress.update(task, description="å„²å­˜è³‡æ–™...")
                fetcher.save_to_json(str(self.labor_law_data_file))

                progress.update(task, description="å®Œæˆï¼")

            self.console.print(f"[OK] æˆåŠŸä¸‹è¼‰ {len(articles)} æ¢å‹åŸºæ³•")
            return True

        except Exception as e:
            self.console.print(f"[FAIL] å‹åŸºæ³•ä¸‹è¼‰å¤±æ•—: {e}")
            return False

    def setup_civil_law_data(self) -> bool:
        """è¨­ç½®æ°‘æ³•è³‡æ–™ï¼ˆä¸‹è¼‰æ³•è¦ï¼‰"""
        if self.civil_law_data_file.exists():
            if not Confirm.ask("æ°‘æ³•è³‡æ–™æª”æ¡ˆå·²å­˜åœ¨ï¼Œæ˜¯å¦é‡æ–°ä¸‹è¼‰ï¼Ÿ"):
                return True

        self.console.print("\n[yellow]é–‹å§‹ä¸‹è¼‰å°ç£æ°‘æ³•...[/yellow]")
        self.console.print("[dim]æ³¨æ„ï¼šæ°‘æ³•æœ‰1229æ¢ï¼Œé è¨ˆéœ€è¦40åˆ†é˜ä»¥ä¸Šæ™‚é–“[/dim]")

        try:
            with Progress(
                SpinnerColumn(),
                TextColumn("[progress.description]{task.description}"),
                console=self.console
            ) as progress:

                task = progress.add_task("ä¸‹è¼‰æ³•è¦å…§å®¹...", total=None)

                fetcher = CivilLawFetcher(delay=2.0)  # è¼ƒé•·å»¶é²ä»¥å°Šé‡ä¼ºæœå™¨
                articles = fetcher.fetch_all_articles()

                progress.update(task, description="å„²å­˜è³‡æ–™...")
                fetcher.save_to_json(str(self.civil_law_data_file))

                progress.update(task, description="å®Œæˆï¼")

            self.console.print(f"[OK] æˆåŠŸä¸‹è¼‰ {len(articles)} æ¢æ°‘æ³•")

            # é¡¯ç¤ºå„ç·¨æ‘˜è¦
            book_summary = fetcher.get_book_summary()
            if book_summary:
                self.console.print("\nå„ç·¨æ‘˜è¦:")
                for book, count in book_summary.items():
                    self.console.print(f"  - {book}: {count} æ¢")

            return True

        except Exception as e:
            self.console.print(f"[FAIL] æ°‘æ³•ä¸‹è¼‰å¤±æ•—: {e}")
            return False

    def build_food_safety_index(self, reset: bool = False) -> bool:
        """å»ºç«‹é£Ÿå“å®‰å…¨æ³•å‘é‡ç´¢å¼•"""
        try:
            # å‚³éç›£æ§å™¨åˆ°ç´¢å¼•å»ºæ§‹å™¨
            self.index_builder = LegalIndexBuilder(
                collection_name="food_safety_act",
                enable_monitoring=self.enable_monitoring,
                monitor=self.monitor
            )

            # æª¢æŸ¥æ˜¯å¦å·²æœ‰ç´¢å¼•
            existing_index = self.index_builder.load_existing_index()

            if existing_index and not reset:
                self.console.print("[OK] æ‰¾åˆ°ç¾æœ‰é£Ÿå“å®‰å…¨æ³•ç´¢å¼•ï¼Œè·³éå»ºç«‹æ­¥é©Ÿ")

                # è¨˜éŒ„è·³éç´¢å¼•å»ºç«‹
                if self.monitor:
                    self.monitor.log_metrics({
                        "food_safety_index_build_skipped": True,
                        "index_exists": True,
                        "reset_requested": reset
                    })

                return True

            self.console.print("\n[yellow]å»ºç«‹é£Ÿå“å®‰å…¨æ³•å‘é‡ç´¢å¼•...[/yellow]")

            with Progress(
                SpinnerColumn(),
                TextColumn("[progress.description]{task.description}"),
                console=self.console
            ) as progress:

                task = progress.add_task("è™•ç†é£Ÿå“å®‰å…¨æ³•æ–‡ä»¶ä¸¦å»ºç«‹ç´¢å¼•...", total=None)

                index = self.index_builder.build_index_from_json(
                    str(self.food_safety_data_file),
                    reset=reset
                )

                progress.update(task, description="é£Ÿå“å®‰å…¨æ³•ç´¢å¼•å»ºç«‹å®Œæˆï¼")

            # é¡¯ç¤ºçµ±è¨ˆè³‡è¨Š
            stats = self.index_builder.get_index_stats()
            self.show_index_stats(stats, "é£Ÿå“å®‰å…¨æ³•")

            return True

        except Exception as e:
            self.console.print(f"[FAIL] é£Ÿå“å®‰å…¨æ³•ç´¢å¼•å»ºç«‹å¤±æ•—: {e}")

            # è¨˜éŒ„ç´¢å¼•å»ºç«‹å¤±æ•—
            if self.monitor:
                self.monitor.log_error(
                    error_type=type(e).__name__,
                    error_message=str(e),
                    context={"operation": "build_food_safety_index", "reset": reset}
                )

            return False

    def build_labor_law_index(self, reset: bool = False) -> bool:
        """å»ºç«‹å‹åŸºæ³•å‘é‡ç´¢å¼•"""
        try:
            if not self.labor_law_data_file.exists():
                self.console.print("[FAIL] å‹åŸºæ³•è³‡æ–™æª”æ¡ˆä¸å­˜åœ¨ï¼Œè«‹å…ˆä¸‹è¼‰è³‡æ–™")
                return False

            self.console.print("\n[yellow]å»ºç«‹å‹åŸºæ³•å‘é‡ç´¢å¼•...[/yellow]")

            with Progress(
                SpinnerColumn(),
                TextColumn("[progress.description]{task.description}"),
                console=self.console
            ) as progress:

                task = progress.add_task("è™•ç†å‹åŸºæ³•æ–‡ä»¶ä¸¦å»ºç«‹ç´¢å¼•...", total=None)

                # ä½¿ç”¨å¢å¼·è™•ç†å™¨
                processor = EnhancedLegalProcessor(chunk_size=512, chunk_overlap=50)

                # è¼‰å…¥å‹åŸºæ³•è³‡æ–™
                with open(self.labor_law_data_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)

                # è™•ç†æ–‡ç« 
                chunks = processor.create_semantic_chunks(data['articles'])
                progress.update(task, description=f"å‰µå»ºäº† {len(chunks)} å€‹èªæ„å¡Š")

                # åˆå§‹åŒ–å‹åŸºæ³•ç´¢å¼•å»ºæ§‹å™¨
                index_builder = LegalIndexBuilder(
                    collection_name="labor_law",
                    enable_monitoring=False  # é¿å…ç·¨ç¢¼å•é¡Œ
                )

                # è½‰æ›ç‚ºæ–‡æª”
                documents = processor.convert_to_llama_documents(chunks)
                progress.update(task, description=f"è½‰æ›äº† {len(documents)} å€‹æ–‡æª”")

                # å»ºç«‹ChromaDBé›†åˆ
                collection = index_builder.create_collection(reset=reset)

                # å»ºç«‹å‘é‡ç´¢å¼•
                from llama_index.vector_stores.chroma import ChromaVectorStore
                from llama_index.core import VectorStoreIndex, StorageContext

                vector_store = ChromaVectorStore(chroma_collection=collection)
                storage_context = StorageContext.from_defaults(vector_store=vector_store)

                index = VectorStoreIndex.from_documents(
                    documents,
                    storage_context=storage_context,
                    show_progress=False
                )

                progress.update(task, description="å‹åŸºæ³•ç´¢å¼•å»ºç«‹å®Œæˆï¼")

            # ä¿å­˜å…ƒæ•¸æ“š
            stats = processor.get_processing_stats(chunks)
            metadata = {
                'law_name': data['law_name'],
                'law_code': data['law_code'],
                'source_url': data['source_url'],
                'total_articles': data['total_articles'],
                'collection_name': 'labor_law',
                'processing_stats': stats
            }

            with open('data/labor_law_index_metadata.json', 'w', encoding='utf-8') as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)

            self.console.print(f"[OK] å‹åŸºæ³•ç´¢å¼•å»ºç«‹å®Œæˆï¼é›†åˆ: labor_lawï¼Œæ–‡æª”: {len(documents)}")

            # é¡¯ç¤ºçµ±è¨ˆè³‡è¨Š
            self.show_enhanced_stats(stats, "å‹åŸºæ³•")

            return True

        except Exception as e:
            self.console.print(f"[FAIL] å‹åŸºæ³•ç´¢å¼•å»ºç«‹å¤±æ•—: {e}")
            import traceback
            self.console.print(traceback.format_exc())
            return False

    def build_civil_law_index(self, reset: bool = False) -> bool:
        """å»ºç«‹æ°‘æ³•å‘é‡ç´¢å¼•"""
        try:
            if not self.civil_law_data_file.exists():
                self.console.print("[FAIL] æ°‘æ³•è³‡æ–™æª”æ¡ˆä¸å­˜åœ¨ï¼Œè«‹å…ˆä¸‹è¼‰è³‡æ–™")
                return False

            self.console.print("\n[yellow]å»ºç«‹æ°‘æ³•å‘é‡ç´¢å¼•...[/yellow]")

            with Progress(
                SpinnerColumn(),
                TextColumn("[progress.description]{task.description}"),
                console=self.console
            ) as progress:

                task = progress.add_task("è™•ç†æ°‘æ³•æ–‡ä»¶ä¸¦å»ºç«‹ç´¢å¼•...", total=None)

                # ä½¿ç”¨å¢å¼·è™•ç†å™¨
                processor = EnhancedLegalProcessor(chunk_size=512, chunk_overlap=50)

                # è¼‰å…¥æ°‘æ³•è³‡æ–™
                with open(self.civil_law_data_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)

                # è™•ç†æ–‡ç« 
                chunks = processor.create_semantic_chunks(data['articles'])
                progress.update(task, description=f"å‰µå»ºäº† {len(chunks)} å€‹èªæ„å¡Š")

                # åˆå§‹åŒ–æ°‘æ³•ç´¢å¼•å»ºæ§‹å™¨
                index_builder = LegalIndexBuilder(
                    collection_name="civil_law",
                    enable_monitoring=False  # é¿å…ç·¨ç¢¼å•é¡Œ
                )

                # è½‰æ›ç‚ºæ–‡æª”
                documents = processor.convert_to_llama_documents(chunks)
                progress.update(task, description=f"è½‰æ›äº† {len(documents)} å€‹æ–‡æª”")

                # å»ºç«‹ChromaDBé›†åˆ
                collection = index_builder.create_collection(reset=reset)

                # å»ºç«‹å‘é‡ç´¢å¼•
                from llama_index.vector_stores.chroma import ChromaVectorStore
                from llama_index.core import VectorStoreIndex, StorageContext

                vector_store = ChromaVectorStore(chroma_collection=collection)
                storage_context = StorageContext.from_defaults(vector_store=vector_store)

                index = VectorStoreIndex.from_documents(
                    documents,
                    storage_context=storage_context,
                    show_progress=False
                )

                progress.update(task, description="æ°‘æ³•ç´¢å¼•å»ºç«‹å®Œæˆï¼")

            # ä¿å­˜å…ƒæ•¸æ“š
            stats = processor.get_processing_stats(chunks)
            metadata = {
                'law_name': data['law_name'],
                'law_code': data['law_code'],
                'source_url': data['source_url'],
                'total_articles': data['total_articles'],
                'collection_name': 'civil_law',
                'processing_stats': stats
            }

            with open('data/civil_law_index_metadata.json', 'w', encoding='utf-8') as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)

            self.console.print(f"[OK] æ°‘æ³•ç´¢å¼•å»ºç«‹å®Œæˆï¼é›†åˆ: civil_lawï¼Œæ–‡æª”: {len(documents)}")

            # é¡¯ç¤ºçµ±è¨ˆè³‡è¨Š
            self.show_enhanced_stats(stats, "æ°‘æ³•")

            return True

        except Exception as e:
            self.console.print(f"[FAIL] æ°‘æ³•ç´¢å¼•å»ºç«‹å¤±æ•—: {e}")
            import traceback
            self.console.print(traceback.format_exc())
            return False

    def initialize_single_rag_system(self) -> bool:
        """åˆå§‹åŒ–å–®ä¸€æ³•è¦RAGç³»çµ±ï¼ˆå‘å¾Œç›¸å®¹ï¼‰"""
        try:
            self.console.print("\n[yellow]åˆå§‹åŒ–å–®ä¸€æ³•è¦RAGæŸ¥è©¢ç³»çµ±...[/yellow]")

            # å‚³éç›£æ§å™¨åˆ° RAG ç³»çµ±
            self.rag_system = LegalRAGSystem(
                enable_monitoring=self.enable_monitoring,
                monitor=self.monitor
            )
            self.rag_system.setup_query_engine(
                similarity_top_k=10,
                similarity_cutoff=0.3
            )

            self.console.print("[OK] å–®ä¸€æ³•è¦RAGç³»çµ±åˆå§‹åŒ–å®Œæˆ")

            # è¨˜éŒ„ RAG ç³»çµ±åˆå§‹åŒ–æˆåŠŸ
            if self.monitor:
                self.monitor.log_metrics({
                    "rag_system_initialized": True,
                    "similarity_top_k": 10,
                    "similarity_cutoff": 0.3
                })

            return True

        except Exception as e:
            self.console.print(f"[FAIL] å–®ä¸€æ³•è¦RAGç³»çµ±åˆå§‹åŒ–å¤±æ•—: {e}")

            # è¨˜éŒ„ RAG ç³»çµ±åˆå§‹åŒ–å¤±æ•—
            if self.monitor:
                self.monitor.log_error(
                    error_type=type(e).__name__,
                    error_message=str(e),
                    context={"operation": "initialize_single_rag_system"}
                )

            return False

    def initialize_multi_domain_system(self) -> bool:
        """åˆå§‹åŒ–å¤šæ³•è¦æ•´åˆç³»çµ±"""
        try:
            self.console.print("\n[yellow]åˆå§‹åŒ–å¤šæ³•è¦æ•´åˆæŸ¥è©¢ç³»çµ±...[/yellow]")

            # åˆå§‹åŒ–æŸ¥è©¢è·¯ç”±å™¨ï¼ˆä½¿ç”¨ç’°å¢ƒè®Šæ•¸è¨­å®šï¼‰
            self.query_router = QueryRouter()

            # åˆå§‹åŒ–é€²éšRAGç³»çµ±ï¼ˆä½¿ç”¨ç’°å¢ƒè®Šæ•¸è¨­å®šï¼‰
            self.advanced_rag_system = AdvancedRAGSystem()

            self.console.print("[OK] å¤šæ³•è¦æ•´åˆç³»çµ±åˆå§‹åŒ–å®Œæˆ")

            # è¨˜éŒ„å¤šæ³•è¦ç³»çµ±åˆå§‹åŒ–æˆåŠŸ
            if self.monitor:
                self.monitor.log_metrics({
                    "multi_domain_system_initialized": True,
                    "query_router_enabled": True,
                    "advanced_rag_enabled": True
                })

            return True

        except Exception as e:
            self.console.print(f"[FAIL] å¤šæ³•è¦æ•´åˆç³»çµ±åˆå§‹åŒ–å¤±æ•—: {e}")

            # è¨˜éŒ„å¤šæ³•è¦ç³»çµ±åˆå§‹åŒ–å¤±æ•—
            if self.monitor:
                self.monitor.log_error(
                    error_type=type(e).__name__,
                    error_message=str(e),
                    context={"operation": "initialize_multi_domain_system"}
                )

            return False

    def show_index_stats(self, stats: dict, law_name: str = ""):
        """é¡¯ç¤ºç´¢å¼•çµ±è¨ˆè³‡è¨Š"""
        title = f"{law_name}ç´¢å¼•çµ±è¨ˆè³‡è¨Š" if law_name else "ç´¢å¼•çµ±è¨ˆè³‡è¨Š"
        table = Table(title=title)
        table.add_column("é …ç›®", style="cyan")
        table.add_column("æ•¸å€¼", style="green")

        # åŸºæœ¬çµ±è¨ˆ
        table.add_row("æ–‡ä»¶ç¸½æ•¸", str(stats.get("document_count", "æœªçŸ¥")))
        table.add_row("åµŒå…¥æ¨¡å‹", stats.get("embedding_model", "æœªçŸ¥"))
        table.add_row("æ³•è¦åç¨±", stats.get("law_name", "æœªçŸ¥"))

        # è™•ç†çµ±è¨ˆ
        if "processing_stats" in stats:
            ps = stats["processing_stats"]
            table.add_row("è™•ç†çš„æ¢æ–‡æ•¸", str(ps.get("articles_processed", "æœªçŸ¥")))
            table.add_row("ç¸½chunkæ•¸", str(ps.get("total_chunks", "æœªçŸ¥")))
            table.add_row("å¹³å‡chunkå¤§å°", f"{ps.get('avg_chunk_size', 0):.1f} tokens")

        self.console.print(table)

    def show_enhanced_stats(self, stats: dict, law_name: str = ""):
        """é¡¯ç¤ºå¢å¼·è™•ç†çµ±è¨ˆè³‡è¨Š"""
        title = f"{law_name}å¢å¼·è™•ç†çµ±è¨ˆ" if law_name else "å¢å¼·è™•ç†çµ±è¨ˆ"
        table = Table(title=title)
        table.add_column("é …ç›®", style="cyan")
        table.add_column("æ•¸å€¼", style="green")

        table.add_row("ç¸½é—œéµå­—æ•¸", str(stats.get("total_keywords", 0)))
        table.add_row("ç¸½æ¦‚å¿µæ•¸", str(stats.get("total_concepts", 0)))
        table.add_row("ç¸½äº¤å‰å¼•ç”¨æ•¸", str(stats.get("total_cross_references", 0)))
        table.add_row("å¹³å‡é‡è¦æ€§åˆ†æ•¸", f"{stats.get('avg_importance_score', 0):.3f}")
        table.add_row("é«˜é‡è¦æ€§å¡Šæ•¸", str(stats.get("high_importance_chunks", 0)))
        table.add_row("å¼•ç”¨åœ–ç¯€é»æ•¸", str(stats.get("graph_nodes", 0)))
        table.add_row("å¼•ç”¨åœ–é‚Šæ•¸", str(stats.get("graph_edges", 0)))

        self.console.print(table)

    def single_domain_query_interface(self):
        """å–®ä¸€æ³•è¦äº’å‹•å¼æŸ¥è©¢ä»‹é¢ï¼ˆå‘å¾Œç›¸å®¹ï¼‰"""
        self.console.print("\n" + "="*60)
        panel_text = "[bold blue]å°ç£æ³•è¦ RAG çŸ¥è­˜æª¢ç´¢ç³»çµ±[/bold blue]\n" \
                    "è¼¸å…¥æ‚¨çš„å•é¡Œï¼Œç³»çµ±å°‡åŸºæ–¼æ³•è¦å…§å®¹ç‚ºæ‚¨è§£ç­”\n" \
                    "[dim]è¼¸å…¥ 'quit' æˆ– 'exit' çµæŸç¨‹å¼[/dim]"

        if self.enable_monitoring and self.monitor:
            panel_text += "\n[dim]ğŸ” W&B ç›£æ§å·²å•Ÿç”¨[/dim]"

        self.console.print(Panel.fit(panel_text, border_style="blue"))

        session_queries = 0
        session_start = time.time()

        while True:
            try:
                # ç²å–ä½¿ç”¨è€…å•é¡Œ
                question = Prompt.ask("\n[bold cyan]è«‹è¼¸å…¥æ‚¨çš„å•é¡Œ[/bold cyan]")

                if question.lower() in ['quit', 'exit', 'é€€å‡º']:
                    break

                if not question.strip():
                    continue

                # åŸ·è¡ŒæŸ¥è©¢
                self.console.print("\n[yellow]ğŸ” æœå°‹ç›¸é—œæ³•è¦...[/yellow]")

                query_start = time.time()
                result = self.rag_system.query(question)
                query_time = time.time() - query_start

                session_queries += 1

                # é¡¯ç¤ºçµæœ
                self.display_query_result(result)

                # è¨˜éŒ„äº’å‹•å¼æŸ¥è©¢çµ±è¨ˆ
                if self.monitor:
                    self.monitor.log_metrics({
                        "session_queries": session_queries,
                        "session_duration": time.time() - session_start,
                        "interactive_mode": True,
                        "query_response_time": query_time
                    })

            except KeyboardInterrupt:
                break
            except Exception as e:
                self.console.print(f"[FAIL] æŸ¥è©¢éŒ¯èª¤: {e}")

                # è¨˜éŒ„æŸ¥è©¢éŒ¯èª¤
                if self.monitor:
                    self.monitor.log_error(
                        error_type=type(e).__name__,
                        error_message=str(e),
                        context={"operation": "interactive_query", "session_queries": session_queries}
                    )

        session_time = time.time() - session_start
        self.console.print("\n[green]æ„Ÿè¬ä½¿ç”¨æ³•è¦çŸ¥è­˜æª¢ç´¢ç³»çµ±ï¼[/green]")

        # è¨˜éŒ„æœƒè©±æ‘˜è¦
        if self.monitor:
            self.monitor.create_summary({
                "session_total_queries": session_queries,
                "session_total_time": session_time,
                "session_avg_query_time": session_time / session_queries if session_queries > 0 else 0,
                "session_type": "single_domain_interactive"
            })

    def multi_domain_query_interface(self):
        """å¤šæ³•è¦æ•´åˆäº’å‹•å¼æŸ¥è©¢ä»‹é¢"""
        self.console.print("\n" + "="*60)
        panel_text = "[bold blue]å°ç£å¤šæ³•è¦æ•´åˆ RAG çŸ¥è­˜æª¢ç´¢ç³»çµ±[/bold blue]\n" \
                    "æ”¯æŒé£Ÿå“å®‰å…¨æ³•ã€å‹åŸºæ³•åŠè·¨æ³•è¦æŸ¥è©¢\n" \
                    "ç³»çµ±å°‡æ™ºèƒ½è·¯ç”±æ‚¨çš„å•é¡Œåˆ°æœ€é©åˆçš„æ³•è¦çŸ¥è­˜åº«\n" \
                    "[dim]è¼¸å…¥ 'quit' æˆ– 'exit' çµæŸç¨‹å¼[/dim]"

        if self.enable_monitoring and self.monitor:
            panel_text += "\n[dim]ğŸ” W&B ç›£æ§å·²å•Ÿç”¨[/dim]"

        self.console.print(Panel.fit(panel_text, border_style="blue"))

        session_queries = 0
        session_start = time.time()

        while True:
            try:
                # ç²å–ä½¿ç”¨è€…å•é¡Œ
                question = Prompt.ask("\n[bold cyan]è«‹è¼¸å…¥æ‚¨çš„å•é¡Œ[/bold cyan]")

                if question.lower() in ['quit', 'exit', 'é€€å‡º']:
                    break

                if not question.strip():
                    continue

                # åŸ·è¡Œå¤šæ³•è¦æŸ¥è©¢
                self.console.print("\n[yellow]ğŸ” æ™ºèƒ½è·¯ç”±æŸ¥è©¢åˆ°ç›¸é—œæ³•è¦...[/yellow]")

                query_start = time.time()
                response = self.query_router.route_query(question, top_k=5)
                query_time = time.time() - query_start

                session_queries += 1

                # é¡¯ç¤ºå¤šæ³•è¦æŸ¥è©¢çµæœ
                self.display_multi_domain_result(response)

                # è¨˜éŒ„äº’å‹•å¼æŸ¥è©¢çµ±è¨ˆ
                if self.monitor:
                    self.monitor.log_metrics({
                        "session_queries": session_queries,
                        "session_duration": time.time() - session_start,
                        "interactive_mode": True,
                        "multi_domain_mode": True,
                        "query_response_time": query_time,
                        "primary_kb": response.route_decision.primary_kb.value if response.route_decision.primary_kb else "conversational",
                        "routing_confidence": response.route_decision.confidence_score
                    })

            except KeyboardInterrupt:
                break
            except Exception as e:
                self.console.print(f"[FAIL] æŸ¥è©¢éŒ¯èª¤: {e}")

                # è¨˜éŒ„æŸ¥è©¢éŒ¯èª¤
                if self.monitor:
                    self.monitor.log_error(
                        error_type=type(e).__name__,
                        error_message=str(e),
                        context={"operation": "multi_domain_interactive_query", "session_queries": session_queries}
                    )

        session_time = time.time() - session_start
        self.console.print("\n[green]æ„Ÿè¬ä½¿ç”¨å¤šæ³•è¦æ•´åˆçŸ¥è­˜æª¢ç´¢ç³»çµ±ï¼[/green]")

        # è¨˜éŒ„æœƒè©±æ‘˜è¦
        if self.monitor:
            self.monitor.create_summary({
                "session_total_queries": session_queries,
                "session_total_time": session_time,
                "session_avg_query_time": session_time / session_queries if session_queries > 0 else 0,
                "session_type": "multi_domain_interactive"
            })

    def display_query_result(self, result):
        """é¡¯ç¤ºæŸ¥è©¢çµæœ"""
        # å›ç­”
        self.console.print(Panel(
            result.answer,
            title="[bold green] æ³•è¦è§£ç­”[/bold green]",
            border_style="green"
        ))

        # ä¿¡å¿ƒåº¦å’ŒæŸ¥è©¢é¡å‹
        confidence_color = "green" if result.confidence_score > 0.7 else "yellow" if result.confidence_score > 0.5 else "red"
        self.console.print(f"\n[{confidence_color}]ä¿¡å¿ƒåº¦: {result.confidence_score:.3f}[/{confidence_color}] | æŸ¥è©¢é¡å‹: [blue]{result.query_type}[/blue]")

        # ç›¸é—œæ³•æ¢
        if result.sources:
            self.console.print("\n[bold cyan]ğŸ“š ç›¸é—œæ³•æ¢:[/bold cyan]")

            sources_table = Table()
            sources_table.add_column("æ¢æ–‡", style="cyan")
            sources_table.add_column("ç›¸ä¼¼åº¦", style="yellow")
            sources_table.add_column("é¡å‹", style="green")
            sources_table.add_column("å…§å®¹é è¦½", style="white")

            for source in result.sources[:3]:  # åªé¡¯ç¤ºå‰3å€‹æœ€ç›¸é—œçš„
                similarity = f"{source['similarity_score']:.3f}"
                preview = source['text_preview'][:100] + "..." if len(source['text_preview']) > 100 else source['text_preview']

                sources_table.add_row(
                    f"ç¬¬{source['article_number']}æ¢",
                    similarity,
                    source['chunk_type'].replace('article_', ''),
                    preview
                )

            self.console.print(sources_table)

    def display_multi_domain_result(self, response):
        """é¡¯ç¤ºå¤šæ³•è¦æŸ¥è©¢çµæœ"""
        # è·¯ç”±è³‡è¨Š
        kb_name = response.route_decision.primary_kb.value if response.route_decision.primary_kb else "conversational"
        self.console.print(f"\n[dim]ğŸ§  æŸ¥è©¢è·¯ç”±: {kb_name} " +
                          f"(ä¿¡å¿ƒåº¦: {response.route_decision.confidence_score:.2f})[/dim]")
        if response.route_decision.secondary_kbs:
            secondary = ", ".join([kb.value for kb in response.route_decision.secondary_kbs])
            self.console.print(f"[dim]æ¬¡è¦æŸ¥è©¢: {secondary}[/dim]")

        # èåˆå›ç­”
        if response.fused_response:
            self.console.print(Panel(
                response.fused_response,
                title="[bold green] æ³•è¦è§£ç­”[/bold green]",
                border_style="green"
            ))
        else:
            self.console.print("[yellow]æœªæ‰¾åˆ°ç›¸é—œè³‡è¨Š[/yellow]")

        # é¡¯ç¤ºå„æ³•è¦çš„æŸ¥è©¢çµæœæ‘˜è¦
        if len(response.responses) > 1:
            # æª¢æŸ¥æ˜¯å¦æœ‰ä»»ä½•æœ‰æ„ç¾©çš„çµæœ
            meaningful_results = []
            total_docs = 0

            for kb_name, kb_response in response.responses.items():
                if "error" not in kb_response:
                    law_name = "å‹åŸºæ³•" if kb_name == "labor_law" else "é£Ÿå“å®‰å…¨æ³•"
                    doc_count = len(kb_response.get('metadata', {}).get('retrieved_nodes', []))
                    answer_preview = kb_response.get('response', '')

                    total_docs += doc_count

                    # æª¢æŸ¥å›ç­”æ˜¯å¦æœ‰æ„ç¾©ï¼ˆä¸æ˜¯é€šç”¨çš„"æ‰¾ä¸åˆ°"è¨Šæ¯ï¼‰
                    is_meaningful = (
                        doc_count > 0 and
                        answer_preview and
                        not answer_preview.startswith("I'm sorry, but based on the provided context") and
                        not answer_preview.startswith("å¾ˆæŠ±æ­‰ï¼Œæ ¹æ“šæä¾›çš„") and
                        len(answer_preview.strip()) > 20
                    )

                    if is_meaningful:
                        meaningful_results.append({
                            'name': law_name,
                            'count': doc_count,
                            'preview': answer_preview[:100] + "..." if len(answer_preview) > 100 else answer_preview
                        })

            # åªæœ‰ç•¶æœ‰æ„ç¾©çš„çµæœæ™‚æ‰é¡¯ç¤ºè¡¨æ ¼
            if meaningful_results:
                self.console.print("\n[bold cyan]ğŸ“ˆ å„æ³•è¦æŸ¥è©¢çµæœ:[/bold cyan]")

                results_table = Table()
                results_table.add_column("æ³•è¦", style="cyan")
                results_table.add_column("ç›¸é—œæ–‡æª”æ•¸", style="yellow")
                results_table.add_column("ç­”æ¡ˆé è¦½", style="white")

                for result in meaningful_results:
                    results_table.add_row(
                        result['name'],
                        str(result['count']),
                        result['preview']
                    )

                self.console.print(results_table)
            elif total_docs == 0:
                # å®Œå…¨æ²’æœ‰æ‰¾åˆ°ç›¸é—œæ–‡æª”æ™‚é¡¯ç¤ºä¸­æ–‡è¨Šæ¯
                self.console.print("\n[yellow]ğŸ“‹ æŸ¥è©¢çµæœ: æŸ¥è©¢ä¸åˆ°ä»»ä½•ç›¸é—œæ³•è¦çµæœ[/yellow]")

    def batch_query_mode(self, questions_file: str):
        """æ‰¹æ¬¡æŸ¥è©¢æ¨¡å¼"""
        try:
            with open(questions_file, 'r', encoding='utf-8') as f:
                questions = [line.strip() for line in f if line.strip()]

            self.console.print(f"[yellow]æ‰¹æ¬¡è™•ç† {len(questions)} å€‹å•é¡Œ...[/yellow]")

            results = []
            with Progress(console=self.console) as progress:
                task = progress.add_task("è™•ç†å•é¡Œ...", total=len(questions))

                for question in questions:
                    result = self.rag_system.query(question)
                    results.append(result)
                    progress.advance(task)

            # å„²å­˜çµæœ
            output_file = f"batch_results_{Path(questions_file).stem}.json"
            self.save_batch_results(results, output_file)

            self.console.print(f"[OK] æ‰¹æ¬¡æŸ¥è©¢å®Œæˆï¼Œçµæœå·²å„²å­˜è‡³ {output_file}")

        except Exception as e:
            self.console.print(f"[FAIL] æ‰¹æ¬¡æŸ¥è©¢å¤±æ•—: {e}")

    def save_batch_results(self, results: List, output_file: str):
        """å„²å­˜æ‰¹æ¬¡æŸ¥è©¢çµæœ"""
        output_data = []
        for result in results:
            output_data.append({
                "question": result.question,
                "answer": result.answer,
                "confidence_score": result.confidence_score,
                "query_type": result.query_type,
                "sources": result.sources
            })

        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)

    def finish_monitoring_session(self):
        """å®Œæˆç›£æ§æœƒè©±"""
        if not self.monitor:
            return

        try:
            # è¨ˆç®—æœƒè©±ç¸½æ™‚é–“
            total_session_time = time.time() - self.session_start_time

            # å»ºç«‹æœ€çµ‚æ‘˜è¦
            final_summary = {
                "total_session_time": total_session_time,
                "session_end_time": datetime.now().isoformat(),
                "monitoring_enabled": self.enable_monitoring
            }

            # å¦‚æœæœ‰ RAG ç³»çµ±ï¼ŒåŠ å…¥å…¶çµ±è¨ˆè³‡è¨Š
            if self.rag_system:
                rag_stats = self.rag_system.get_system_stats()
                final_summary.update({
                    "total_queries_processed": rag_stats.get("total_queries", 0),
                    "avg_query_time": rag_stats.get("avg_query_time", 0.0)
                })

            self.monitor.create_summary(final_summary)
            self.monitor.finish_run()
            self.console.print("[dim]W&B ç›£æ§æœƒè©±å·²çµæŸ[/dim]")

        except Exception as e:
            self.console.print(f"[yellow]å®Œæˆç›£æ§æœƒè©±æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}[/yellow]")

    def show_all_domain_stats(self):
        """é¡¯ç¤ºæ‰€æœ‰æ³•è¦ç´¢å¼•çµ±è¨ˆè³‡è¨Š"""
        self.console.print("\n[bold blue]æ‰€æœ‰æ³•è¦ç´¢å¼•çµ±è¨ˆè³‡è¨Š[/bold blue]")

        # é£Ÿå“å®‰å…¨æ³•çµ±è¨ˆ
        if self.food_safety_data_file.exists():
            try:
                index_builder = LegalIndexBuilder(
                    collection_name="food_safety_act",
                    enable_monitoring=False
                )
                if index_builder.load_existing_index():
                    stats = index_builder.get_index_stats()
                    self.show_index_stats(stats, "é£Ÿå“å®‰å…¨æ³•")
                else:
                    self.console.print("[yellow]é£Ÿå“å®‰å…¨æ³•ç´¢å¼•ä¸å­˜åœ¨[/yellow]")
            except Exception as e:
                self.console.print(f"[yellow]ç„¡æ³•è¼‰å…¥é£Ÿå“å®‰å…¨æ³•ç´¢å¼•: {e}[/yellow]")
        else:
            self.console.print("[yellow]é£Ÿå“å®‰å…¨æ³•è³‡æ–™ä¸å­˜åœ¨[/yellow]")

        # å‹åŸºæ³•çµ±è¨ˆ
        if Path("data/labor_law_index_metadata.json").exists():
            try:
                with open('data/labor_law_index_metadata.json', 'r', encoding='utf-8') as f:
                    metadata = json.load(f)
                self.show_enhanced_stats(metadata.get('processing_stats', {}), "å‹åŸºæ³•")
            except Exception as e:
                self.console.print(f"[yellow]ç„¡æ³•è¼‰å…¥å‹åŸºæ³•çµ±è¨ˆ: {e}[/yellow]")
        else:
            self.console.print("[yellow]å‹åŸºæ³•ç´¢å¼•ä¸å­˜åœ¨[/yellow]")

        # æ°‘æ³•çµ±è¨ˆ
        if Path("data/civil_law_index_metadata.json").exists():
            try:
                with open('data/civil_law_index_metadata.json', 'r', encoding='utf-8') as f:
                    metadata = json.load(f)
                self.show_enhanced_stats(metadata.get('processing_stats', {}), "æ°‘æ³•")
            except Exception as e:
                self.console.print(f"[yellow]ç„¡æ³•è¼‰å…¥æ°‘æ³•çµ±è¨ˆ: {e}[/yellow]")
        else:
            self.console.print("[yellow]æ°‘æ³•ç´¢å¼•ä¸å­˜åœ¨[/yellow]")

    def run(self, args):
        """ä¸»è¦åŸ·è¡Œé‚è¼¯"""
        self.console.print("[bold blue]å°ç£æ³•è¦ RAG çŸ¥è­˜æª¢ç´¢ç³»çµ±[/bold blue]\n")

        # åˆå§‹åŒ–ç›£æ§
        if self.enable_monitoring:
            self.setup_monitoring()

        # ç’°å¢ƒæª¢æŸ¥
        if not self.check_environment():
            if Confirm.ask("æ˜¯å¦è¦é€²è¡Œåˆå§‹è¨­ç½®ï¼Ÿ"):
                self.console.print("\n[yellow]è«‹å…ˆå®Œæˆä»¥ä¸‹è¨­ç½®ï¼š[/yellow]")
                self.console.print("1. è¤‡è£½ .env.template ç‚º .env")
                self.console.print("2. åœ¨ .env ä¸­è¨­å®šæ‚¨çš„ OPENAI_API_KEY")
                self.console.print("3. é‡æ–°åŸ·è¡Œç¨‹å¼")
            return

        # è™•ç†æ‰€æœ‰çµ±è¨ˆè³‡è¨Šé¡¯ç¤º
        if args.show_all_stats:
            self.show_all_domain_stats()
            return

        # è™•ç†é£Ÿå“å®‰å…¨æ³•è³‡æ–™åŠç´¢å¼•
        if args.fetch_food_data or (not self.food_safety_data_file.exists() and not args.multi_domain and not args.domain == 'labor'):
            if not self.setup_food_safety_data():
                return

        # è™•ç†å‹åŸºæ³•è³‡æ–™åŠç´¢å¼•
        if args.fetch_labor_data:
            if not self.setup_labor_law_data():
                return

        # è™•ç†æ°‘æ³•è³‡æ–™åŠç´¢å¼•
        if args.fetch_civil_data:
            if not self.setup_civil_law_data():
                return

        # å»ºç«‹é£Ÿå“å®‰å…¨æ³•ç´¢å¼•
        if args.rebuild_food_index or (not Path("chroma_db").exists() and (not args.multi_domain and args.domain != 'labor')):
            if not self.build_food_safety_index(reset=args.rebuild_food_index):
                return

        # å»ºç«‹å‹åŸºæ³•ç´¢å¼•
        if args.rebuild_labor_index:
            if not self.build_labor_law_index(reset=True):
                return

        # å»ºç«‹æ°‘æ³•ç´¢å¼•
        if args.rebuild_civil_index:
            if not self.build_civil_law_index(reset=True):
                return

        # é¸æ“‡é‹è¡Œæ¨¡å¼
        if args.multi_domain:
            # å¤šæ³•è¦æ•´åˆæ¨¡å¼
            if not self.initialize_multi_domain_system():
                return

            # åŸ·è¡Œå°æ‡‰æ¨¡å¼
            if args.batch_file:
                self.multi_domain_batch_query_mode(args.batch_file)
            elif args.query:
                response = self.query_router.route_query(args.query, top_k=5)
                self.display_multi_domain_result(response)

                # è¨˜éŒ„å–®ä¸€æŸ¥è©¢æ¨¡å¼
                if self.monitor:
                    self.monitor.create_summary({
                        "session_type": "multi_domain_single_query",
                        "query_text": args.query[:100] + "..." if len(args.query) > 100 else args.query,
                        "primary_kb": response.route_decision.primary_kb.value if response.route_decision.primary_kb else "conversational",
                        "routing_confidence": response.route_decision.confidence_score
                    })
            else:
                self.multi_domain_query_interface()
        else:
            # å–®ä¸€æ³•è¦æ¨¡å¼ï¼ˆå‘å¾Œç›¸å®¹ï¼‰
            if not self.initialize_single_rag_system():
                return

            # åŸ·è¡Œå°æ‡‰æ¨¡å¼
            if args.batch_file:
                self.batch_query_mode(args.batch_file)
            elif args.query:
                result = self.rag_system.query(args.query)
                self.display_query_result(result)

                # è¨˜éŒ„å–®ä¸€æŸ¥è©¢æ¨¡å¼
                if self.monitor:
                    self.monitor.create_summary({
                        "session_type": "single_query",
                        "query_text": args.query[:100] + "..." if len(args.query) > 100 else args.query
                    })
            else:
                self.single_domain_query_interface()

    def multi_domain_batch_query_mode(self, questions_file: str):
        """å¤šæ³•è¦æ‰¹æ¬¡æŸ¥è©¢æ¨¡å¼"""
        try:
            with open(questions_file, 'r', encoding='utf-8') as f:
                questions = [line.strip() for line in f if line.strip()]

            self.console.print(f"[yellow]å¤šæ³•è¦æ‰¹æ¬¡è™•ç† {len(questions)} å€‹å•é¡Œ...[/yellow]")

            results = []
            with Progress(console=self.console) as progress:
                task = progress.add_task("è™•ç†å•é¡Œ...", total=len(questions))

                for question in questions:
                    response = self.query_router.route_query(question, top_k=5)
                    results.append(response)
                    progress.advance(task)

            # å„²å­˜çµæœ
            output_file = f"multi_domain_batch_results_{Path(questions_file).stem}.json"
            self.save_multi_domain_batch_results(results, output_file)

            self.console.print(f"[OK] å¤šæ³•è¦æ‰¹æ¬¡æŸ¥è©¢å®Œæˆï¼Œçµæœå·²å„²å­˜è‡³ {output_file}")

        except Exception as e:
            self.console.print(f"[FAIL] å¤šæ³•è¦æ‰¹æ¬¡æŸ¥è©¢å¤±æ•—: {e}")

    def save_multi_domain_batch_results(self, results, output_file: str):
        """å„²å­˜å¤šæ³•è¦æ‰¹æ¬¡æŸ¥è©¢çµæœ"""
        output_data = []
        for response in results:
            output_data.append({
                "question": response.query,
                "primary_kb": response.route_decision.primary_kb.value if response.route_decision.primary_kb else "conversational",
                "confidence_score": response.route_decision.confidence_score,
                "reasoning": response.route_decision.reasoning,
                "fused_response": response.fused_response,
                "responses": {kb: resp for kb, resp in response.responses.items() if "error" not in resp},
                "metadata": response.metadata
            })

        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)

        # å®Œæˆç›£æ§æœƒè©±
        self.finish_monitoring_session()


def main():
    """ä¸»ç¨‹å¼å…¥å£"""
    parser = argparse.ArgumentParser(
        description="å°ç£æ³•è¦ RAG çŸ¥è­˜æª¢ç´¢ç³»çµ± - æ”¯æŒé£Ÿå“å®‰å…¨æ³•ã€å‹åŸºæ³•åŠå¤šæ³•è¦æ•´åˆæŸ¥è©¢ (å« W&B ç›£æ§)",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¯„ä¾‹:
  # åŸºæœ¬ä½¿ç”¨
  python main.py                                    # å–®ä¸€æ³•è¦äº’å‹•å¼æŸ¥è©¢
  python main.py --multi-domain                     # å¤šæ³•è¦æ•´åˆäº’å‹•å¼æŸ¥è©¢
  python main.py -q "é£Ÿå“æ·»åŠ ç‰©çš„é™åˆ¶ï¼Ÿ"            # å–®ä¸€æŸ¥è©¢
  python main.py --multi-domain -q "å‹å·¥é£Ÿå“å®‰å…¨è¦å®š"  # å¤šæ³•è¦å–®ä¸€æŸ¥è©¢

  # è³‡æ–™ç®¡ç†
  python main.py --fetch-food-data                  # ä¸‹è¼‰é£Ÿå“å®‰å…¨æ³•è³‡æ–™
  python main.py --fetch-labor-data                 # ä¸‹è¼‰å‹åŸºæ³•è³‡æ–™
  python main.py --fetch-civil-data                 # ä¸‹è¼‰æ°‘æ³•è³‡æ–™
  python main.py --rebuild-food-index               # é‡å»ºé£Ÿå“å®‰å…¨æ³•ç´¢å¼•
  python main.py --rebuild-labor-index              # é‡å»ºå‹åŸºæ³•ç´¢å¼•
  python main.py --rebuild-civil-index              # é‡å»ºæ°‘æ³•ç´¢å¼•

  # æ‰¹æ¬¡è™•ç†å’Œçµ±è¨ˆ
  python main.py --batch questions.txt              # å–®ä¸€æ³•è¦æ‰¹æ¬¡æŸ¥è©¢
  python main.py --multi-domain --batch questions.txt # å¤šæ³•è¦æ‰¹æ¬¡æŸ¥è©¢
  python main.py --all-stats                        # é¡¯ç¤ºæ‰€æœ‰æ³•è¦çµ±è¨ˆ
  python main.py --no-monitoring                    # åœç”¨ W&B ç›£æ§
        """
    )

    # æŸ¥è©¢ç›¸é—œåƒæ•¸
    parser.add_argument("-q", "--query",
                       help="åŸ·è¡Œå–®ä¸€æŸ¥è©¢")
    parser.add_argument("--multi-domain", action="store_true",
                       help="å•Ÿç”¨å¤šæ³•è¦æ•´åˆæ¨¡å¼ï¼Œæ”¯æŒæ™ºèƒ½è·¯ç”±å’Œè·¨æ³•è¦æŸ¥è©¢")
    parser.add_argument("--domain", choices=["food", "labor", "civil", "all"], default="food",
                       help="æŒ‡å®šæŸ¥è©¢çš„æ³•è¦é ˜åŸŸ (é è¨­: food)")

    # è³‡æ–™ç®¡ç†åƒæ•¸
    parser.add_argument("--fetch-food-data", action="store_true",
                       help="ä¸‹è¼‰é£Ÿå“å®‰å…¨è¡›ç”Ÿç®¡ç†æ³•è³‡æ–™")
    parser.add_argument("--fetch-labor-data", action="store_true",
                       help="ä¸‹è¼‰å‹å‹•åŸºæº–æ³•è³‡æ–™")
    parser.add_argument("--fetch-civil-data", action="store_true",
                       help="ä¸‹è¼‰å°ç£æ°‘æ³•è³‡æ–™")
    parser.add_argument("--rebuild-food-index", action="store_true",
                       help="é‡å»ºé£Ÿå“å®‰å…¨æ³•å‘é‡ç´¢å¼•")
    parser.add_argument("--rebuild-labor-index", action="store_true",
                       help="é‡å»ºå‹åŸºæ³•å‘é‡ç´¢å¼•")
    parser.add_argument("--rebuild-civil-index", action="store_true",
                       help="é‡å»ºæ°‘æ³•å‘é‡ç´¢å¼•")

    # æ‰¹æ¬¡è™•ç†å’Œçµ±è¨ˆåƒæ•¸
    parser.add_argument("--batch", dest="batch_file",
                       help="æ‰¹æ¬¡æŸ¥è©¢æª”æ¡ˆè·¯å¾‘")
    parser.add_argument("--all-stats", dest="show_all_stats", action="store_true",
                       help="é¡¯ç¤ºæ‰€æœ‰æ³•è¦ç´¢å¼•çµ±è¨ˆè³‡è¨Š")

    # ç³»çµ±åƒæ•¸
    parser.add_argument("--no-monitoring", action="store_true",
                       help="åœç”¨ W&B ç›£æ§")

    # å‘å¾Œç›¸å®¹åƒæ•¸
    parser.add_argument("--fetch-data", action="store_true",
                       help="(å·²å»¢æ£„) è«‹ä½¿ç”¨ --fetch-food-data")
    parser.add_argument("--rebuild-index", action="store_true",
                       help="(å·²å»¢æ£„) è«‹ä½¿ç”¨ --rebuild-food-index")
    parser.add_argument("--stats", dest="show_stats", action="store_true",
                       help="(å·²å»¢æ£„) è«‹ä½¿ç”¨ --all-stats")

    args = parser.parse_args()

    # è™•ç†å‘å¾Œç›¸å®¹åƒæ•¸
    if args.fetch_data:
        args.fetch_food_data = True
        print("[WARNING] --fetch-data å·²å»¢æ£„ï¼Œè‡ªå‹•è½‰æ›ç‚º --fetch-food-data")
    if args.rebuild_index:
        args.rebuild_food_index = True
        print("[WARNING] --rebuild-index å·²å»¢æ£„ï¼Œè‡ªå‹•è½‰æ›ç‚º --rebuild-food-index")
    if args.show_stats:
        args.show_all_stats = True
        print("[WARNING] --stats å·²å»¢æ£„ï¼Œè‡ªå‹•è½‰æ›ç‚º --all-stats")

    # æª¢æŸ¥Pythonç‰ˆæœ¬
    if sys.version_info < (3, 8):
        print("[FAIL] éœ€è¦ Python 3.8 æˆ–æ›´é«˜ç‰ˆæœ¬")
        sys.exit(1)

    try:
        cli = LegalRAGCLI(enable_monitoring=not args.no_monitoring)
        cli.run(args)
    except KeyboardInterrupt:
        print("\nç¨‹å¼å·²ä¸­æ­¢")
    except Exception as e:
        print(f"[FAIL] ç¨‹å¼éŒ¯èª¤: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()