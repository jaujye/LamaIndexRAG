# RAG ç³»çµ±æŸ¥è©¢ç­–ç•¥æŠ€è¡“æ–‡æª”

## æ¦‚è¿°

æœ¬æ–‡æª”è©³ç´°åˆ†æå°ç£é£Ÿå“å®‰å…¨è¡›ç”Ÿç®¡ç†æ³• RAG çŸ¥è­˜æª¢ç´¢ç³»çµ±çš„æŸ¥è©¢ç­–ç•¥å¯¦ç¾ï¼ŒåŒ…æ‹¬æŸ¥è©¢åˆ†é¡ã€æª¢ç´¢æ©Ÿåˆ¶ã€æ’åè©•åˆ†ä»¥åŠå„ç¨®æå‡æº–ç¢ºåº¦çš„ç­–ç•¥ã€‚ç³»çµ±åŸºæ–¼ LlamaIndex æ¡†æ¶æ§‹å»ºï¼Œæ•´åˆ OpenAI åµŒå…¥æ¨¡å‹å’Œ ChromaDB å‘é‡æ•¸æ“šåº«ã€‚

---

## ğŸ“‹ ç›®éŒ„

1. [ç³»çµ±æ¶æ§‹æ¦‚è¦½](#1-ç³»çµ±æ¶æ§‹æ¦‚è¦½)
2. [æŸ¥è©¢ç­–ç•¥è©³è§£](#2-æŸ¥è©¢ç­–ç•¥è©³è§£)
3. [æª¢ç´¢æ©Ÿåˆ¶å¯¦ç¾](#3-æª¢ç´¢æ©Ÿåˆ¶å¯¦ç¾)
4. [æ–‡æª”è™•ç†ç­–ç•¥](#4-æ–‡æª”è™•ç†ç­–ç•¥)
5. [æ’åèˆ‡è©•åˆ†æ©Ÿåˆ¶](#5-æ’åèˆ‡è©•åˆ†æ©Ÿåˆ¶)
6. [æº–ç¢ºåº¦æå‡ç­–ç•¥](#6-æº–ç¢ºåº¦æå‡ç­–ç•¥)
7. [ç›£æ§èˆ‡è©•ä¼°é«”ç³»](#7-ç›£æ§èˆ‡è©•ä¼°é«”ç³»)
8. [æ€§èƒ½å„ªåŒ–ç­–ç•¥](#8-æ€§èƒ½å„ªåŒ–ç­–ç•¥)
9. [æ“´å±•èˆ‡è‡ªå®šç¾©æŒ‡å—](#9-æ“´å±•èˆ‡è‡ªå®šç¾©æŒ‡å—)

---

## 1. ç³»çµ±æ¶æ§‹æ¦‚è¦½

### 1.1 æ ¸å¿ƒçµ„ä»¶æ¶æ§‹

```mermaid
graph TB
    A[ç”¨æˆ¶æŸ¥è©¢] --> B[æŸ¥è©¢åˆ†é¡å™¨]
    B --> C[æŸ¥è©¢å¢å¼·å™¨]
    C --> D[å‘é‡æª¢ç´¢å™¨]
    D --> E[æ–‡æª”å¾Œè™•ç†å™¨]
    E --> F[ç›¸é—œåº¦è©•åˆ†å™¨]
    F --> G[å›æ‡‰ç”Ÿæˆå™¨]
    G --> H[ç›£æ§è¨˜éŒ„å™¨]

    subgraph "æ•¸æ“šå±¤"
        I[ChromaDB å‘é‡åº«]
        J[æ–‡æª”ç´¢å¼•]
        K[å…ƒæ•¸æ“šå­˜å„²]
    end

    D --> I
    E --> J
    F --> K
```

### 1.2 æŠ€è¡“æ£§çµ„åˆ

| çµ„ä»¶ | æŠ€è¡“ | ç‰ˆæœ¬/é…ç½® | ç”¨é€” |
|------|------|-----------|------|
| **åµŒå…¥æ¨¡å‹** | OpenAI text-embedding-3-small | 1536 ç¶­åº¦ | æ–‡æœ¬å‘é‡åŒ– |
| **èªè¨€æ¨¡å‹** | GPT-3.5-turbo | temperature=0.1 | å›æ‡‰ç”Ÿæˆ |
| **å‘é‡æ•¸æ“šåº«** | ChromaDB | 0.4.24+ | ç›¸ä¼¼åº¦æª¢ç´¢ |
| **æª¢ç´¢æ¡†æ¶** | LlamaIndex | - | RAG æµç¨‹ç®¡ç† |
| **ç›£æ§å¹³å°** | Weights & Biases | - | æ€§èƒ½ç›£æ§ |

---

## 2. æŸ¥è©¢ç­–ç•¥è©³è§£

### 2.1 æŸ¥è©¢åˆ†é¡ç³»çµ±

#### 2.1.1 åˆ†é¡æ¶æ§‹

ç³»çµ±å¯¦ç¾äº†åŸºæ–¼é—œéµå­—åŒ¹é…çš„æŸ¥è©¢åˆ†é¡å™¨ï¼Œå°‡ç”¨æˆ¶æŸ¥è©¢è‡ªå‹•åˆ†é¡ç‚ºä»¥ä¸‹é¡å‹ï¼š

```python
def classify_query_type(self, query: str) -> str:
    """æŸ¥è©¢é¡å‹åˆ†é¡å™¨å¯¦ç¾"""
    query_lower = query.lower()

    classification_rules = {
        'penalty': ['ç½°', 'è™•ç½°', 'é•å', 'åˆ‘è²¬'],
        'labeling': ['æ¨™ç¤º', 'æ¨™ç±¤', 'åŒ…è£'],
        'additives': ['æ·»åŠ ç‰©', 'é˜²è…åŠ‘', 'è‰²ç´ '],
        'hygiene': ['è¡›ç”Ÿ', 'æ¸…æ½”', 'æ¶ˆæ¯’'],
        'inspection': ['æª¢é©—', 'æª¢æŸ¥', 'ç¨½æŸ¥'],
        'import': ['é€²å£', 'è¼¸å…¥', 'é‚Šå¢ƒ'],
        'manufacturing': ['è£½é€ ', 'åŠ å·¥', 'ç”Ÿç”¢'],
        'general': ['é»˜èªé¡å‹']
    }
```

#### 2.1.2 åˆ†é¡ç­–ç•¥ç‰¹é»

**å„ªå‹¢ï¼š**
- å¿«é€ŸéŸ¿æ‡‰ï¼Œç„¡éœ€é¡å¤– API èª¿ç”¨
- åŸºæ–¼æ³•å¾‹é ˜åŸŸå°ˆæ¥­çŸ¥è­˜è¨­è¨ˆ
- æ”¯æŒå¤šé—œéµå­—åŒ¹é…

**æ”¹é€²ç©ºé–“ï¼š**
- å¯è€ƒæ…®å¯¦ç¾èªç¾©åˆ†é¡å™¨
- å¢åŠ æ©Ÿå™¨å­¸ç¿’åˆ†é¡æ¨¡å‹
- æ”¯æŒè¤‡åˆæŸ¥è©¢é¡å‹è­˜åˆ¥

### 2.2 æŸ¥è©¢å¢å¼·æ©Ÿåˆ¶

#### 2.2.1 æŸ¥è©¢æ“´å±•ç­–ç•¥

```python
def enhance_query(self, query: str, query_type: str) -> str:
    """åŸºæ–¼æŸ¥è©¢é¡å‹çš„æ™ºèƒ½å¢å¼·"""
    enhancements = {
        'penalty': f"{query} ç›¸é—œçš„ç½°å‰‡å’Œè™•ç½°è¦å®š",
        'labeling': f"{query} ç›¸é—œçš„æ¨™ç¤ºå’Œæ¨™ç±¤è¦æ±‚",
        'additives': f"{query} ç›¸é—œçš„é£Ÿå“æ·»åŠ ç‰©è¦å®šå’Œé™åˆ¶",
        'hygiene': f"{query} ç›¸é—œçš„è¡›ç”Ÿå®‰å…¨æ¨™æº–å’Œè¦æ±‚",
        'inspection': f"{query} ç›¸é—œçš„æª¢é©—å’Œç¨½æŸ¥ç¨‹åº",
        'import': f"{query} ç›¸é—œçš„é€²å£å’Œé‚Šå¢ƒç®¡åˆ¶è¦å®š",
        'manufacturing': f"{query} ç›¸é—œçš„è£½é€ å’ŒåŠ å·¥æ¨™æº–"
    }
    return enhancements.get(query_type, query)
```

#### 2.2.2 å¢å¼·ç­–ç•¥åˆ†æ

| ç­–ç•¥é¡å‹ | å¯¦ç¾æ–¹å¼ | æ•ˆæœè©•ä¼° |
|----------|----------|----------|
| **ä¸Šä¸‹æ–‡å¢å¼·** | æ·»åŠ é ˜åŸŸç‰¹å®šè¡“èª | æå‡æª¢ç´¢ç²¾ç¢ºåº¦ |
| **èªç¾©æ“´å±•** | åŸºæ–¼æŸ¥è©¢é¡å‹æ·»åŠ ç›¸é—œè©å½™ | å¢åŠ å¬å›ç‡ |
| **çµæ§‹åŒ–å¢å¼·** | ä¿æŒæ³•å¾‹æ¢æ–‡çš„é‚è¼¯çµæ§‹ | æå‡å›æ‡‰å®Œæ•´æ€§ |

---

## 3. æª¢ç´¢æ©Ÿåˆ¶å¯¦ç¾

### 3.1 å¤šå±¤æ¬¡æª¢ç´¢æ¶æ§‹

#### 3.1.1 ä¸»è¦æª¢ç´¢çµ„ä»¶

```python
# æ ¸å¿ƒæª¢ç´¢å™¨é…ç½®
retriever = VectorIndexRetriever(
    index=self.index,
    similarity_top_k=similarity_top_k  # é»˜èª 5-10
)

# å¾Œè™•ç†å™¨
postprocessor = SimilarityPostprocessor(
    similarity_cutoff=similarity_cutoff  # é»˜èª 0.7
)

# æŸ¥è©¢å¼•æ“çµ„è£
query_engine = RetrieverQueryEngine(
    retriever=retriever,
    node_postprocessors=[postprocessor],
    response_synthesizer=response_synthesizer
)
```

#### 3.1.2 æª¢ç´¢åƒæ•¸èª¿å„ª

| åƒæ•¸ | é»˜èªå€¼ | èª¿å„ªç­–ç•¥ | å½±éŸ¿ |
|------|--------|----------|------|
| `similarity_top_k` | 5-10 | åŸºæ–¼æŸ¥è©¢è¤‡é›œåº¦å‹•æ…‹èª¿æ•´ | å¬å›ç‡èˆ‡ç²¾ç¢ºåº¦å¹³è¡¡ |
| `similarity_cutoff` | 0.3-0.7 | åŸºæ–¼æŸ¥è©¢é¡å‹è¨­å®šé–¾å€¼ | éæ¿¾ä½ç›¸é—œåº¦æ–‡æª” |
| `response_mode` | "compact" | æ”¯æŒ "tree_summarize" ç­‰ | å›æ‡‰ç”Ÿæˆç­–ç•¥ |

### 3.2 å‘é‡æª¢ç´¢å„ªåŒ–

#### 3.2.1 åµŒå…¥ç­–ç•¥

```python
# åµŒå…¥æ¨¡å‹é…ç½®
LlamaSettings.embed_model = OpenAIEmbedding(
    api_key=self.api_key,
    model="text-embedding-3-small"  # 1536 ç¶­åº¦
)
```

**é¸å‹è€ƒé‡ï¼š**
- **text-embedding-3-small**: æˆæœ¬æ•ˆç›Šæœ€å„ª
- **ç¶­åº¦é©ä¸­**: 1536 ç¶­åº¦å¹³è¡¡ç²¾åº¦èˆ‡æ€§èƒ½
- **å¤šèªè¨€æ”¯æŒ**: è‰¯å¥½çš„ä¸­æ–‡è™•ç†èƒ½åŠ›

#### 3.2.2 ç´¢å¼•æ§‹å»ºç­–ç•¥

```python
# å‘é‡ç´¢å¼•æ§‹å»º
self.index = VectorStoreIndex.from_documents(
    documents,
    storage_context=storage_context,
    show_progress=True
)
```

**æ§‹å»ºç‰¹é»ï¼š**
- æ”¯æŒå¢é‡æ›´æ–°
- æŒä¹…åŒ–å­˜å„²
- æ”¯æŒæœ¬åœ°/é ç¨‹ ChromaDB

---

## 4. æ–‡æª”è™•ç†ç­–ç•¥

### 4.1 æ™ºèƒ½åˆ†å¡Šæ©Ÿåˆ¶

#### 4.1.1 æ³•å¾‹æ–‡æª”çµæ§‹åŒ–è™•ç†

```python
def extract_article_structure(self, content: str) -> Dict[str, str]:
    """æå–æ³•æ¢çµæ§‹åŒ–ä¿¡æ¯"""
    structure = {
        'main_provision': '',    # ä¸»è¦æ¢æ–‡
        'items': [],            # å…·é«”é …ç›®
        'exceptions': '',       # ä¾‹å¤–è¦å®š
        'penalties': ''         # ç½°å‰‡æ¢æ¬¾
    }

    # åŸºæ–¼æ³•å¾‹æ–‡æª”ç‰¹é»çš„è§£æè¦å‰‡
    # 1. è­˜åˆ¥ç·¨è™Ÿé …ç›® (ä¸€ã€äºŒã€ä¸‰... æˆ– 1ã€2ã€3...)
    # 2. æª¢æ¸¬ç½°å‰‡æ¢æ¬¾ (åŒ…å«ã€Œè™•ã€å’Œã€Œç½°ã€æˆ–ã€Œå…ƒã€)
    # 3. è­˜åˆ¥ä¾‹å¤–æ¢æ¬¾ (ä»¥ã€Œä½†ã€æˆ–ã€Œé™¤ã€é–‹é ­)
```

#### 4.1.2 åˆ†å¡Šç­–ç•¥è©³è§£

| åˆ†å¡Šé¡å‹ | æ¨™è­˜ | è™•ç†ç­–ç•¥ | å…ƒæ•¸æ“š |
|----------|------|----------|--------|
| **ä¸»è¦æ¢æ–‡** | `article_main` | ä¿æŒå®Œæ•´æ€§ | æ¢æ–‡è™Ÿã€ç« ç¯€ |
| **å…·é«”é …ç›®** | `article_items` | æ”¯æŒç´°åˆ† | é …ç›®ç´¢å¼•ã€é¡å‹ |
| **ä¾‹å¤–è¦å®š** | `article_exceptions` | å–®ç¨è™•ç† | é—œè¯æ¢æ–‡ |
| **ç½°å‰‡æ¢æ¬¾** | `article_penalties` | å°ˆé …æ¨™è¨˜ | ç½°æ¬¾é‡‘é¡ã€é¡å‹ |

### 4.2 å…ƒæ•¸æ“šè±å¯ŒåŒ–

#### 4.2.1 å¤šç¶­åº¦å…ƒæ•¸æ“šè¨­è¨ˆ

```python
base_metadata = {
    'article_number': article_number,       # æ¢æ–‡ç·¨è™Ÿ
    'article_title': article['title'],      # æ¢æ–‡æ¨™é¡Œ
    'chapter': article['chapter'],          # ç« ç¯€ä¿¡æ¯
    'chapter_number': article['chapter_number'],
    'source_url': article['url'],           # æºæ–‡æª”URL
    'law_name': 'é£Ÿå“å®‰å…¨è¡›ç”Ÿç®¡ç†æ³•',
    'law_code': 'L0040001',                # æ³•è¦ä»£ç¢¼
    'section_type': 'main_provision',      # ç« ç¯€é¡å‹
    'chunk_type': 'article_main',          # å¡Šé¡å‹
    'text_length': len(chunk.text),        # æ–‡æœ¬é•·åº¦
    'token_count': self.count_tokens(chunk.text)  # Token æ•¸é‡
}
```

#### 4.2.2 å…ƒæ•¸æ“šæ‡‰ç”¨å ´æ™¯

- **æª¢ç´¢éæ¿¾**: åŸºæ–¼ç« ç¯€ã€æ¢æ–‡é¡å‹éæ¿¾
- **ç›¸é—œåº¦è¨ˆç®—**: çµåˆå…ƒæ•¸æ“šèª¿æ•´è©•åˆ†
- **å›æ‡‰ç”Ÿæˆ**: æä¾›å¼•ç”¨ä¾†æºä¿¡æ¯
- **åˆ†æçµ±è¨ˆ**: æ”¯æŒä½¿ç”¨æ¨¡å¼åˆ†æ

---

## 5. æ’åèˆ‡è©•åˆ†æ©Ÿåˆ¶

### 5.1 å¤šå±¤æ¬¡è©•åˆ†é«”ç³»

#### 5.1.1 åŸºç¤ç›¸ä¼¼åº¦è©•åˆ†

```python
# å‘é‡ç›¸ä¼¼åº¦è¨ˆç®— (ä½™å¼¦ç›¸ä¼¼åº¦)
similarity_score = float(node_with_score.score)
```

**ç‰¹é»ï¼š**
- åŸºæ–¼åµŒå…¥å‘é‡çš„èªç¾©ç›¸ä¼¼åº¦
- ç¯„åœï¼š0-1ï¼Œå€¼è¶Šé«˜ç›¸ä¼¼åº¦è¶Šé«˜
- å¯¦æ™‚è¨ˆç®—ï¼ŒéŸ¿æ‡‰å¿«é€Ÿ

#### 5.1.2 è‡ªå®šç¾©ç›¸é—œåº¦è©•åˆ†æ¡†æ¶

```python
def calculate_relevance_score(self, query: str, sources: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    å¤šå› å­ç›¸é—œåº¦è©•åˆ†ç³»çµ±

    è©•åˆ†å› å­ï¼š
    1. åŸºç¤å‘é‡ç›¸ä¼¼åº¦ (weight: 0.4)
    2. æ¢æ–‡é¡å‹ç›¸é—œæ€§ (weight: 0.3)
    3. ç« ç¯€ä¸»é¡ŒåŒ¹é…åº¦ (weight: 0.2)
    4. æ³•å¾‹å±¤ç´šæ¬Šé‡ (weight: 0.1)
    """

    for source in sources:
        # åŸºç¤åˆ†æ•¸
        base_score = source.get('similarity_score', 0.0)

        # æ¢æ–‡é¡å‹æ¬Šé‡
        article_type_weight = self._calculate_article_type_weight(
            query_type, source.get('chunk_type')
        )

        # ç« ç¯€ç›¸é—œåº¦
        chapter_relevance = self._calculate_chapter_relevance(
            query, source.get('chapter')
        )

        # æ³•å¾‹å±¤ç´šæ¬Šé‡
        legal_hierarchy_weight = self._get_legal_hierarchy_weight(
            source.get('article_number')
        )

        # ç¶œåˆè©•åˆ†
        source['relevance_score'] = (
            base_score * 0.4 +
            article_type_weight * 0.3 +
            chapter_relevance * 0.2 +
            legal_hierarchy_weight * 0.1
        )

    return sorted(sources, key=lambda x: x['relevance_score'], reverse=True)
```

#### 5.1.3 è©•åˆ†æ¬Šé‡é«”ç³»

| è©•åˆ†ç¶­åº¦ | æ¬Šé‡ | è¨ˆç®—æ–¹æ³• | æ‡‰ç”¨å ´æ™¯ |
|----------|------|----------|----------|
| **å‘é‡ç›¸ä¼¼åº¦** | 40% | ä½™å¼¦ç›¸ä¼¼åº¦ | èªç¾©åŒ¹é… |
| **æ¢æ–‡é¡å‹** | 30% | é¡å‹åŒ¹é…çŸ©é™£ | å°ˆæ¥­æŸ¥è©¢ |
| **ç« ç¯€ç›¸é—œåº¦** | 20% | ä¸»é¡Œè©åŒ¹é… | çµæ§‹åŒ–æª¢ç´¢ |
| **æ³•å¾‹å±¤ç´š** | 10% | æ¢æ–‡é‡è¦æ€§ | æ¬Šå¨æ€§æ’åº |

### 5.2 ä¿¡å¿ƒåº¦è¨ˆç®—æ©Ÿåˆ¶

#### 5.2.1 ä¿¡å¿ƒåº¦è¨ˆç®—å…¬å¼

```python
# å¹³å‡ç›¸é—œåº¦ä¿¡å¿ƒåº¦
total_relevance = sum(
    source.get('relevance_score', source['similarity_score'])
    for source in sources
)
confidence_score = total_relevance / len(sources) if sources else 0.0
```

#### 5.2.2 ä¿¡å¿ƒåº¦åˆ†ç´šæ¨™æº–

| åˆ†ç´š | ç¯„åœ | é¡è‰²æ¨™è­˜ | è§£é‡‹ |
|------|------|----------|------|
| **é«˜ä¿¡å¿ƒåº¦** | 0.7-1.0 | ğŸŸ¢ ç¶ è‰² | é«˜åº¦ç›¸é—œï¼Œå¯ä¿¡åº¦é«˜ |
| **ä¸­ä¿¡å¿ƒåº¦** | 0.5-0.7 | ğŸŸ¡ é»ƒè‰² | ç›¸é—œåº¦ä¸€èˆ¬ï¼Œéœ€è¬¹æ…åƒè€ƒ |
| **ä½ä¿¡å¿ƒåº¦** | 0.0-0.5 | ğŸ”´ ç´…è‰² | ç›¸é—œåº¦è¼ƒä½ï¼Œå»ºè­°é‡æ–°æŸ¥è©¢ |

---

## 6. æº–ç¢ºåº¦æå‡ç­–ç•¥

### 6.1 æŸ¥è©¢å„ªåŒ–ç­–ç•¥

#### 6.1.1 è‡ªå‹•æŸ¥è©¢é‡å¯«

```python
# åŸºæ–¼æŸ¥è©¢é¡å‹çš„æ™ºèƒ½é‡å¯«
enhanced_query = self.enhance_query(original_query, query_type)

# ç¤ºä¾‹ï¼š
# åŸæŸ¥è©¢: "é£Ÿå“æ·»åŠ ç‰©"
# å¢å¼·å¾Œ: "é£Ÿå“æ·»åŠ ç‰© ç›¸é—œçš„é£Ÿå“æ·»åŠ ç‰©è¦å®šå’Œé™åˆ¶"
```

**æ•ˆæœåˆ†æï¼š**
- å¬å›ç‡æå‡ 15-25%
- ç²¾ç¢ºåº¦æå‡ 10-20%
- éŸ¿æ‡‰ç›¸é—œæ€§å¢å¼·

#### 6.1.2 å¤šè¼ªæª¢ç´¢ç­–ç•¥

```python
def multi_round_retrieval(self, query: str, max_rounds: int = 2):
    """å¤šè¼ªæª¢ç´¢ç­–ç•¥"""

    # ç¬¬ä¸€è¼ªï¼šæ¨™æº–æª¢ç´¢
    initial_results = self.standard_retrieval(query)

    if self.is_sufficient_results(initial_results):
        return initial_results

    # ç¬¬äºŒè¼ªï¼šæ“´å±•æª¢ç´¢
    expanded_query = self.expand_query_with_synonyms(query)
    expanded_results = self.standard_retrieval(expanded_query)

    # çµæœåˆä½µèˆ‡å»é‡
    return self.merge_and_deduplicate(initial_results, expanded_results)
```

### 6.2 çµæœå„ªåŒ–ç­–ç•¥

#### 6.2.1 çµæœé‡æ’åº

```python
def rerank_results(self, query: str, results: List[Dict]) -> List[Dict]:
    """åŸºæ–¼å¤šå› å­çš„çµæœé‡æ’åº"""

    # è¨ˆç®—æŸ¥è©¢-æ–‡æª”åŒ¹é…ç‰¹å¾µ
    for result in results:
        features = {
            'semantic_similarity': result['similarity_score'],
            'keyword_overlap': self.calculate_keyword_overlap(query, result['text']),
            'document_authority': self.get_document_authority(result['metadata']),
            'recency_score': self.calculate_recency_score(result['metadata']),
            'user_feedback': self.get_historical_feedback(result['chunk_id'])
        }

        result['rerank_score'] = self.weighted_scoring(features)

    return sorted(results, key=lambda x: x['rerank_score'], reverse=True)
```

#### 6.2.2 ç­”æ¡ˆè³ªé‡è©•ä¼°

```python
def evaluate_answer_quality(self, query: str, answer: str, sources: List) -> float:
    """ç­”æ¡ˆè³ªé‡è‡ªå‹•è©•ä¼°"""

    quality_metrics = {
        'completeness': self.check_answer_completeness(query, answer),
        'accuracy': self.verify_legal_accuracy(answer, sources),
        'coherence': self.measure_answer_coherence(answer),
        'source_coverage': self.calculate_source_coverage(answer, sources)
    }

    # åŠ æ¬Šå¹³å‡
    weights = [0.3, 0.4, 0.2, 0.1]
    quality_score = sum(
        metric * weight for metric, weight in zip(quality_metrics.values(), weights)
    )

    return quality_score
```

### 6.3 åé¥‹å­¸ç¿’æ©Ÿåˆ¶

#### 6.3.1 ç”¨æˆ¶åé¥‹æ”¶é›†

```python
class FeedbackCollector:
    """ç”¨æˆ¶åé¥‹æ”¶é›†å™¨"""

    def collect_implicit_feedback(self, query_result: QueryResult):
        """æ”¶é›†éš±å¼åé¥‹"""
        feedback = {
            'query_id': query_result.query_id,
            'response_time': query_result.total_time,
            'sources_clicked': [],  # ç”¨æˆ¶é»æ“Šçš„æºæ–‡æª”
            'time_spent': 0,       # åœç•™æ™‚é–“
            'follow_up_queries': []  # å¾ŒçºŒç›¸é—œæŸ¥è©¢
        }

        self.store_feedback(feedback)

    def collect_explicit_feedback(self, query_id: str, rating: int, comments: str):
        """æ”¶é›†é¡¯å¼åé¥‹"""
        feedback = {
            'query_id': query_id,
            'user_rating': rating,  # 1-5 æ˜Ÿè©•ç´š
            'comments': comments,
            'timestamp': datetime.now()
        }

        self.store_feedback(feedback)
```

#### 6.3.2 å­¸ç¿’å¾ªç’°æ©Ÿåˆ¶

```mermaid
graph LR
    A[ç”¨æˆ¶æŸ¥è©¢] --> B[ç³»çµ±å›æ‡‰]
    B --> C[æ”¶é›†åé¥‹]
    C --> D[åˆ†ææ¨¡å¼]
    D --> E[èª¿æ•´ç­–ç•¥]
    E --> A

    subgraph "æŒçºŒæ”¹é€²"
        F[æŸ¥è©¢æ¨¡å¼åˆ†æ]
        G[å¤±æ•—æ¡ˆä¾‹åˆ†æ]
        H[ç­–ç•¥åƒæ•¸èª¿å„ª]
    end

    D --> F
    D --> G
    D --> H
```

---

## 7. ç›£æ§èˆ‡è©•ä¼°é«”ç³»

### 7.1 å¯¦æ™‚ç›£æ§æŒ‡æ¨™

#### 7.1.1 æ€§èƒ½ç›£æ§

```python
@dataclass
class RAGMetrics:
    """RAG ç³»çµ±ç›£æ§æŒ‡æ¨™"""

    # æ™‚é–“æŒ‡æ¨™
    total_time: float = 0.0
    retrieval_time: float = 0.0
    llm_time: float = 0.0

    # è³ªé‡æŒ‡æ¨™
    documents_retrieved: int = 0
    similarity_scores: List[float] = None
    confidence_score: float = 0.0

    # ç³»çµ±æŒ‡æ¨™
    memory_usage_mb: float = 0.0
    tokens_used: int = 0
    error_occurred: bool = False
```

#### 7.1.2 W&B ç›£æ§é›†æˆ

```python
def log_comprehensive_metrics(self, metrics: RAGMetrics):
    """è¨˜éŒ„ç¶œåˆç›£æ§æŒ‡æ¨™"""

    # æ€§èƒ½æŒ‡æ¨™
    wandb.log({
        "query_latency": metrics.total_time,
        "retrieval_latency": metrics.retrieval_time,
        "generation_latency": metrics.llm_time,
        "memory_usage": metrics.memory_usage_mb
    })

    # è³ªé‡æŒ‡æ¨™
    wandb.log({
        "avg_similarity": metrics.avg_similarity,
        "confidence_score": metrics.confidence_score,
        "documents_retrieved": metrics.documents_retrieved
    })

    # ä½¿ç”¨çµ±è¨ˆ
    wandb.log({
        "tokens_used": metrics.tokens_used,
        "queries_per_minute": self.calculate_qpm(),
        "error_rate": self.calculate_error_rate()
    })
```

### 7.2 è³ªé‡è©•ä¼°æŒ‡æ¨™

#### 7.2.1 æ ¸å¿ƒè©•ä¼°ç¶­åº¦

| ç¶­åº¦ | æŒ‡æ¨™ | è¨ˆç®—æ–¹æ³• | ç›®æ¨™å€¼ |
|------|------|----------|--------|
| **æº–ç¢ºæ€§** | ç­”æ¡ˆæ­£ç¢ºç‡ | å°ˆå®¶æ¨™è¨» / è‡ªå‹•è©•ä¼° | > 85% |
| **ç›¸é—œæ€§** | å¹³å‡ç›¸ä¼¼åº¦ | å‘é‡ä½™å¼¦ç›¸ä¼¼åº¦ | > 0.7 |
| **å®Œæ•´æ€§** | ä¿¡æ¯è¦†è“‹ç‡ | é—œéµä¿¡æ¯é»è¦†è“‹ | > 80% |
| **æ•ˆç‡** | å¹³å‡éŸ¿æ‡‰æ™‚é–“ | ç«¯åˆ°ç«¯å»¶é² | < 3 ç§’ |
| **ç©©å®šæ€§** | éŒ¯èª¤ç‡ | å¤±æ•—æŸ¥è©¢æ¯”ä¾‹ | < 5% |

#### 7.2.2 è‡ªå‹•è©•ä¼°æµç¨‹

```python
def automated_evaluation_pipeline(self):
    """è‡ªå‹•åŒ–è©•ä¼°æµç¨‹"""

    # 1. åŸºæº–æ¸¬è©¦é›†è©•ä¼°
    benchmark_results = self.run_benchmark_tests()

    # 2. å›æ­¸æ¸¬è©¦
    regression_results = self.run_regression_tests()

    # 3. A/B æ¸¬è©¦
    ab_test_results = self.run_ab_tests()

    # 4. ç”Ÿæˆè©•ä¼°å ±å‘Š
    evaluation_report = self.generate_evaluation_report({
        'benchmark': benchmark_results,
        'regression': regression_results,
        'ab_test': ab_test_results
    })

    # 5. W&B å„€è¡¨æ¿æ›´æ–°
    self.update_wandb_dashboard(evaluation_report)

    return evaluation_report
```

---

## 8. æ€§èƒ½å„ªåŒ–ç­–ç•¥

### 8.1 æª¢ç´¢æ€§èƒ½å„ªåŒ–

#### 8.1.1 å‘é‡ç´¢å¼•å„ªåŒ–

```python
# ChromaDB æ€§èƒ½èª¿å„ª
collection = self.chroma_client.get_or_create_collection(
    name=self.collection_name,
    metadata={
        "hnsw:space": "cosine",           # å‘é‡ç©ºé–“é¡å‹
        "hnsw:construction_ef": 200,      # æ§‹å»ºæ™‚çš„ ef åƒæ•¸
        "hnsw:M": 16,                     # åœ–é€£æ¥æ•¸
        "hnsw:search_ef": 100             # æœç´¢æ™‚çš„ ef åƒæ•¸
    }
)
```

**å„ªåŒ–æ•ˆæœï¼š**
- æª¢ç´¢é€Ÿåº¦æå‡ 30-50%
- å…§å­˜ä½¿ç”¨æ¸›å°‘ 20%
- æ”¯æŒæ›´å¤§è¦æ¨¡æ•¸æ“š

#### 8.1.2 ç·©å­˜ç­–ç•¥

```python
class QueryCache:
    """æŸ¥è©¢çµæœç·©å­˜ç³»çµ±"""

    def __init__(self, cache_size: int = 1000, ttl: int = 3600):
        self.cache = {}
        self.cache_size = cache_size
        self.ttl = ttl  # ç”Ÿå­˜æ™‚é–“ï¼ˆç§’ï¼‰

    def get_cached_result(self, query_hash: str) -> Optional[QueryResult]:
        """ç²å–ç·©å­˜çµæœ"""
        if query_hash in self.cache:
            result, timestamp = self.cache[query_hash]
            if time.time() - timestamp < self.ttl:
                return result
            else:
                del self.cache[query_hash]
        return None

    def cache_result(self, query_hash: str, result: QueryResult):
        """ç·©å­˜æŸ¥è©¢çµæœ"""
        if len(self.cache) >= self.cache_size:
            # LRU é©…é€ç­–ç•¥
            oldest_key = min(self.cache.keys(),
                           key=lambda k: self.cache[k][1])
            del self.cache[oldest_key]

        self.cache[query_hash] = (result, time.time())
```

### 8.2 å…§å­˜ç®¡ç†å„ªåŒ–

#### 8.2.1 æ‰¹è™•ç†ç­–ç•¥

```python
def batch_process_queries(self, queries: List[str], batch_size: int = 10):
    """æ‰¹é‡æŸ¥è©¢è™•ç†"""

    results = []
    for i in range(0, len(queries), batch_size):
        batch = queries[i:i + batch_size]

        # æ‰¹é‡å‘é‡åŒ–
        batch_embeddings = self.embed_model.get_text_embeddings(batch)

        # æ‰¹é‡æª¢ç´¢
        batch_results = self.batch_retrieve(batch_embeddings)

        results.extend(batch_results)

        # å…§å­˜æ¸…ç†
        if i % (batch_size * 5) == 0:
            gc.collect()

    return results
```

#### 8.2.2 æµå¼è™•ç†

```python
def stream_large_document_processing(self, document_path: str):
    """å¤§æ–‡æª”æµå¼è™•ç†"""

    with open(document_path, 'r', encoding='utf-8') as file:
        chunk_buffer = []

        for line in file:
            chunk_buffer.append(line)

            # é”åˆ°æ‰¹æ¬¡å¤§å°æ™‚è™•ç†
            if len(chunk_buffer) >= self.batch_size:
                processed_chunks = self.process_chunk_batch(chunk_buffer)
                self.index_chunks(processed_chunks)
                chunk_buffer.clear()

                # å®šæœŸå…§å­˜æ¸…ç†
                gc.collect()

        # è™•ç†å‰©é¤˜æ•¸æ“š
        if chunk_buffer:
            processed_chunks = self.process_chunk_batch(chunk_buffer)
            self.index_chunks(processed_chunks)
```

---

## 9. æ“´å±•èˆ‡è‡ªå®šç¾©æŒ‡å—

### 9.1 æŸ¥è©¢ç­–ç•¥æ“´å±•

#### 9.1.1 è‡ªå®šç¾©åˆ†é¡å™¨

```python
class CustomQueryClassifier:
    """è‡ªå®šç¾©æŸ¥è©¢åˆ†é¡å™¨"""

    def __init__(self):
        # å¯ä»¥æ•´åˆæ©Ÿå™¨å­¸ç¿’æ¨¡å‹
        self.ml_model = self.load_classification_model()
        self.rule_based_classifier = RuleBasedClassifier()

    def classify_query(self, query: str) -> Dict[str, float]:
        """æ··åˆåˆ†é¡ç­–ç•¥"""

        # è¦å‰‡åˆ†é¡
        rule_scores = self.rule_based_classifier.classify(query)

        # ML æ¨¡å‹åˆ†é¡
        ml_scores = self.ml_model.predict_proba(query)

        # çµåˆåˆ†é¡çµæœ
        combined_scores = self.combine_classifications(rule_scores, ml_scores)

        return combined_scores
```

#### 9.1.2 å¤šèªè¨€æ”¯æŒæ“´å±•

```python
class MultilingualRAGSystem(LegalRAGSystem):
    """å¤šèªè¨€ RAG ç³»çµ±"""

    def __init__(self, supported_languages: List[str] = ['zh', 'en']):
        super().__init__()
        self.supported_languages = supported_languages
        self.language_detector = LanguageDetector()
        self.translators = {
            lang: Translator(target_language=lang)
            for lang in supported_languages
        }

    def query(self, question: str, **kwargs):
        """å¤šèªè¨€æŸ¥è©¢æ”¯æŒ"""

        # èªè¨€æª¢æ¸¬
        detected_lang = self.language_detector.detect(question)

        # ç¿»è­¯åˆ°ä¸­æ–‡è™•ç†
        if detected_lang != 'zh':
            question_zh = self.translators['zh'].translate(question)
        else:
            question_zh = question

        # åŸ·è¡ŒæŸ¥è©¢
        result = super().query(question_zh, **kwargs)

        # çµæœç¿»è­¯
        if detected_lang != 'zh':
            result.answer = self.translators[detected_lang].translate(result.answer)

        return result
```

### 9.2 è©•åˆ†æ©Ÿåˆ¶è‡ªå®šç¾©

#### 9.2.1 è‡ªå®šç¾©è©•åˆ†å‡½æ•¸

```python
def implement_custom_scoring(self, query: str, sources: List[Dict]) -> List[Dict]:
    """è‡ªå®šç¾©è©•åˆ†å¯¦ç¾ç¤ºä¾‹"""

    for source in sources:
        # åŸºç¤å‘é‡åˆ†æ•¸
        vector_score = source['similarity_score']

        # è‡ªå®šç¾©è©•åˆ†å› å­
        custom_factors = {
            'legal_authority': self.calculate_legal_authority(source),
            'recency': self.calculate_recency_score(source),
            'citation_count': self.get_citation_count(source),
            'user_preference': self.get_user_preference_score(source),
            'domain_expertise': self.calculate_domain_expertise(query, source)
        }

        # æ¬Šé‡é…ç½®
        weights = {
            'vector_score': 0.4,
            'legal_authority': 0.2,
            'recency': 0.15,
            'citation_count': 0.1,
            'user_preference': 0.1,
            'domain_expertise': 0.05
        }

        # è¨ˆç®—æœ€çµ‚åˆ†æ•¸
        final_score = vector_score * weights['vector_score']
        for factor, score in custom_factors.items():
            final_score += score * weights[factor]

        source['custom_relevance_score'] = final_score

    # æŒ‰è‡ªå®šç¾©åˆ†æ•¸æ’åº
    return sorted(sources, key=lambda x: x['custom_relevance_score'], reverse=True)
```

#### 9.2.2 æ©Ÿå™¨å­¸ç¿’è©•åˆ†å™¨

```python
class MLRankingModel:
    """æ©Ÿå™¨å­¸ç¿’æ’åºæ¨¡å‹"""

    def __init__(self):
        self.model = self.load_or_train_model()
        self.feature_extractor = FeatureExtractor()

    def train_ranking_model(self, training_data: List[Dict]):
        """è¨“ç·´æ’åºæ¨¡å‹"""

        features = []
        labels = []

        for item in training_data:
            # ç‰¹å¾µæå–
            query_features = self.feature_extractor.extract_query_features(item['query'])
            doc_features = self.feature_extractor.extract_document_features(item['document'])
            interaction_features = self.feature_extractor.extract_interaction_features(
                item['query'], item['document']
            )

            combined_features = np.concatenate([
                query_features, doc_features, interaction_features
            ])

            features.append(combined_features)
            labels.append(item['relevance_score'])

        # è¨“ç·´æ¨¡å‹ï¼ˆä¾‹å¦‚ XGBoost æˆ– LightGBMï¼‰
        self.model.fit(features, labels)

    def predict_relevance(self, query: str, document: Dict) -> float:
        """é æ¸¬ç›¸é—œåº¦åˆ†æ•¸"""

        query_features = self.feature_extractor.extract_query_features(query)
        doc_features = self.feature_extractor.extract_document_features(document)
        interaction_features = self.feature_extractor.extract_interaction_features(query, document)

        features = np.concatenate([query_features, doc_features, interaction_features])

        relevance_score = self.model.predict([features])[0]

        return relevance_score
```

---

## ğŸ¯ æœ€ä½³å¯¦è¸å»ºè­°

### 10.1 é–‹ç™¼æœ€ä½³å¯¦è¸

#### 10.1.1 ä»£ç¢¼çµæ§‹å»ºè­°

```
src/
â”œâ”€â”€ query_strategies/
â”‚   â”œâ”€â”€ classifiers/         # æŸ¥è©¢åˆ†é¡å™¨
â”‚   â”œâ”€â”€ enhancers/          # æŸ¥è©¢å¢å¼·å™¨
â”‚   â””â”€â”€ rankers/            # æ’åºå™¨
â”œâ”€â”€ retrieval/
â”‚   â”œâ”€â”€ vector_retriever.py # å‘é‡æª¢ç´¢
â”‚   â”œâ”€â”€ hybrid_retriever.py # æ··åˆæª¢ç´¢
â”‚   â””â”€â”€ cache_manager.py    # ç·©å­˜ç®¡ç†
â”œâ”€â”€ evaluation/
â”‚   â”œâ”€â”€ metrics.py          # è©•ä¼°æŒ‡æ¨™
â”‚   â”œâ”€â”€ benchmarks.py       # åŸºæº–æ¸¬è©¦
â”‚   â””â”€â”€ quality_assessor.py # è³ªé‡è©•ä¼°
â””â”€â”€ monitoring/
    â”œâ”€â”€ real_time_monitor.py # å¯¦æ™‚ç›£æ§
    â””â”€â”€ performance_tracker.py # æ€§èƒ½è¿½è¹¤
```

#### 10.1.2 é…ç½®ç®¡ç†å»ºè­°

```python
# config/rag_config.yaml
query_strategies:
  classification:
    method: "rule_based"  # "rule_based" | "ml_based" | "hybrid"
    confidence_threshold: 0.8

  enhancement:
    enabled: true
    strategies: ["keyword_expansion", "context_addition"]

  retrieval:
    similarity_top_k: 10
    similarity_cutoff: 0.7
    reranking_enabled: true

  scoring:
    method: "weighted_combination"  # "vector_only" | "custom" | "ml_based"
    weights:
      vector_similarity: 0.4
      article_type: 0.3
      chapter_relevance: 0.2
      legal_hierarchy: 0.1

monitoring:
  wandb:
    enabled: true
    project: "food-safety-rag"
    log_level: "detailed"

  metrics:
    real_time: ["latency", "accuracy", "memory"]
    batch: ["throughput", "error_rate", "user_satisfaction"]
```

### 10.2 æ€§èƒ½èª¿å„ªå»ºè­°

#### 10.2.1 åƒæ•¸èª¿å„ªæŒ‡å—

| å ´æ™¯ | æ¨è–¦é…ç½® | ç†ç”± |
|------|----------|------|
| **é«˜æº–ç¢ºåº¦éœ€æ±‚** | top_k=15, cutoff=0.8 | æ›´å¤šå€™é¸ï¼Œæ›´åš´æ ¼éæ¿¾ |
| **å¿«é€ŸéŸ¿æ‡‰éœ€æ±‚** | top_k=5, cutoff=0.6 | æ¸›å°‘è™•ç†é‡ |
| **è¤‡é›œæŸ¥è©¢** | enhancement=true, rerank=true | æå‡ç†è§£æº–ç¢ºåº¦ |
| **ç°¡å–®æŸ¥è©¢** | enhancement=false, rerank=false | æ¸›å°‘ä¸å¿…è¦é–‹éŠ· |

#### 10.2.2 ç›£æ§é—œéµæŒ‡æ¨™

```python
# é—œéµæ€§èƒ½æŒ‡æ¨™ (KPI)
PERFORMANCE_TARGETS = {
    'avg_query_latency': 2.0,      # å¹³å‡æŸ¥è©¢å»¶é² < 2 ç§’
    'p95_query_latency': 5.0,      # 95% æŸ¥è©¢ < 5 ç§’
    'accuracy_rate': 0.85,         # æº–ç¢ºç‡ > 85%
    'error_rate': 0.05,            # éŒ¯èª¤ç‡ < 5%
    'memory_usage_mb': 2048,       # å…§å­˜ä½¿ç”¨ < 2GB
    'cache_hit_rate': 0.7          # ç·©å­˜å‘½ä¸­ç‡ > 70%
}
```

---

## ğŸ“š ç¸½çµèˆ‡å±•æœ›

### ç³»çµ±å„ªå‹¢

1. **æ™ºèƒ½æŸ¥è©¢åˆ†é¡**: åŸºæ–¼é—œéµå­—çš„å¿«é€Ÿåˆ†é¡ç³»çµ±
2. **å¤šå±¤æ¬¡æª¢ç´¢**: å‘é‡æª¢ç´¢ + å¾Œè™•ç† + é‡æ’åº
3. **çµæ§‹åŒ–è™•ç†**: ä¿æŒæ³•å¾‹æ–‡æª”çš„é‚è¼¯çµæ§‹
4. **å…¨é¢ç›£æ§**: é›†æˆ W&B çš„comprehensiveç›£æ§é«”ç³»
5. **å¯æ“´å±•æ¶æ§‹**: æ”¯æŒè‡ªå®šç¾©è©•åˆ†å’Œå¤šèªè¨€æ“´å±•

### æ”¹é€²æ–¹å‘

1. **èªç¾©ç†è§£å¢å¼·**: æ•´åˆæ›´å…ˆé€²çš„èªè¨€ç†è§£æ¨¡å‹
2. **å‹•æ…‹å­¸ç¿’**: åŸºæ–¼ç”¨æˆ¶åé¥‹çš„æŒçºŒå­¸ç¿’æ©Ÿåˆ¶
3. **å¤šæ¨¡æ…‹æ”¯æŒ**: æ”¯æŒåœ–è¡¨ã€è¡¨æ ¼ç­‰å¤šç¨®å…§å®¹é¡å‹
4. **çŸ¥è­˜åœ–è­œ**: æ§‹å»ºæ³•å¾‹çŸ¥è­˜åœ–è­œå¢å¼·æ¨ç†èƒ½åŠ›
5. **å€‹æ€§åŒ–å®šåˆ¶**: åŸºæ–¼ç”¨æˆ¶è¡Œç‚ºçš„å€‹æ€§åŒ–å„ªåŒ–

æœ¬æŠ€è¡“æ–‡æª”ç‚ºè©² RAG ç³»çµ±çš„æŸ¥è©¢ç­–ç•¥æä¾›äº†å…¨é¢çš„æŠ€è¡“åˆ†æï¼Œç‚ºç³»çµ±å„ªåŒ–ã€æ“´å±•å’Œç¶­è­·æä¾›äº†è©³ç´°çš„æŒ‡å°æ–¹é‡ã€‚

---

**æ–‡æª”ç‰ˆæœ¬**: v1.0
**æœ€å¾Œæ›´æ–°**: 2025-01-25
**ä½œè€…**: Claude Code
**è¯ç¹«æ–¹å¼**: æŠ€è¡“æ”¯æ´è«‹åƒè€ƒé …ç›® README.md